## [Designing and Implementing a Data Science Solution on Azure(DP-100) - Azure Data Scientist Associate](#top)

- [Azure ML workspace](#azure-ml-workspace)
  - [Access to Azure Machine learning workspace](#access-to-azure-machine-learning-workspace)
  - [Azure ML resources](#azure-ml-resources)
  - [Azure ML assets](#azure-ml-assets)
  - [Train Models in the workspace](#train-models-in-the-workspace)
  - [Workspace Tasks](#Workspace-Tasks)
- [Developer tools for workspace interaction](#developer-tools-for-workspace-interaction)
  - [Azure ML Studio](#azure-ml-studio)
  - [Python SDK](#python-sdk)
  - [Work with Azure CLI](#work-with-azure-cli)
  - [Other tools](#Other-tools)
- [Make data available in Azure Machine Learning](#make-data-available-in-azure-machine-learning)
   - [Types of datastores](#types-of-datastores)
  - [URIs](#uris)
  - [Create datastore to an Azure Blob Storage](#create-datastore-to-an-azure-blob-storage)
  - [Create a data asset](#create-a-data-asset)
- [Automated machine learning](#automated-machine-learning)
- [Run a training script as a command job in Azure Machine Learning](#run-a-training-script-as-a-command-job-in-azure-machine-learning)
- [Optimize model training with pipelines in Azure Machine Learning(Machine Learning Operations-MLOps)](#optimize-model-training-with-pipelines-in-azure-machine-learningmachine-learning-operations-mlops)
  - [component](#component)
  - [Pipeline](#pipeline)
- [Azure ML Models](#azure-ml-models)
  - [Steps for training model](#steps-for-training-model)
  - [Regression Models](#regression-models)
  - [Classification models](#classification-models)
  - [Clustering models](#clustering-models)
- [Project Jupyter \& Notebooks](#project-jupyter--notebooks)
- [Azure Data Platform Services](#azure-data-platform-services)
- [Azure Storage Accounts](#azure-storage-accounts)
  - [Azure Blob storage](#azure-blob-storage)
  - [Azure File Shares](#azure-file-shares)
  - [Azure Table storgae](#azure-table-storgae)
  - [Azure Queue storgae](#azure-queue-storgae)
- [Azure Storage Strategy](#azure-storage-strategy)
  - [Data structure type](#data-structure-type)
  - [Azure storage security](#azure-storage-security)
  - [Console application demo by using Visual Studio Code](#console-application-demo-by-using-visual-studio-code)
  - [Azure Storage Services](#azure-storage-services)
- [Azure Data Factory](#azure-data-factory)
  - [Azure Data Factory Elements](#azure-data-factory-elements)
  - [Azure Data Factory Integration Runtime](#azure-data-factory-integration-runtime)
  - [Azure Data Factory Triggers](#azure-data-factory-triggers)
- [Non-relational Data Stores](#non-relational-data-stores)
    - [Azure Cosmos DB](#azure-cosmos-db)
    - [Azure Blob storage](#azure-blob-storage)
    - [Azure Data Lake Storage Gen2](#azure-data-lake-storage-gen2)
    - [Azure Data Masking](#azure-data-masking)
    - [Azure Data Encryption at Rest](#azure-data-encryption-at-rest)

--------------------------------------------------------------------

### Azure ML workspace

![Azure-ML-workspace1](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace1.png)
![Azure-ML-workspace2](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace2.png)
![Azure-ML-workspace3](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace3.png)

#### Access to Azure Machine learning workspace

- Three general built-in roles
  - Owner: full access to all resources, can grant access to others
  - Contributor: full access to all resources, can't grant access to others
  - Reader: only view the resources
- Azure machine learning's specific built-in roles
  - AzureML Data Scientist
  - AzureML compute Operator
- to fully customize permissions, create a custom role

#### Azure ML resources

- Workspace:  top-level for ML, stores all logs, metrics, outputs, models and snapshots
- Compute resources:
  - compute instance: similar to VM in the cloud, managed by workspace
  - compute cluster: on-demand clusters of CPU or FPU compute nodes in the cloud
  - inference clutster: allows to create or attach Azue Kubernetes Service(AKS) cluster Idela to deploy trained ML models in production scenarios
  - attached compute: allow to attach other Azure compute resources suck as Databricks or Synapse Spark pools
- Datastores: all data is stored in datastores which references to azure data services. When connected to the workspace, two datastores will added to your workspace: **workspaceFilestore** and **workspaceblobstore**

#### Azure ML assets

- Models:
- Environments: stored as an image in Azure Container Registry created with workspace when it's used for the first time
- Data: can use data assets to easily access data every time, without having to provide authentication every time. When create a data assets in the workspace, you'll specify the path to point to the file or folder, name and version.
- Components: make it easier to share code with component in a workspace

#### Train Models in the workspace

- Author and run a pipeline with the designer
- Automated ML: explore algorithms and hyperparameter values
- Jupyter notebook:
  - files:  will stored in the file share of storage account
  - run notebook: use a compute instance
  - can edit and run notebook in Visual Studio Code
- Running a pipeline with the designer
- Run a script as a job
  - command
  - Sweep: perform hyperparameter tuning when executing a single script
  - Pipeline: rung a pipeline consisting of multiple scripts or components

#### Workspace Tasks

- Run experiment to train model
- User automated ML to train model
- Register a model
- Deploy a model
- Create and run workflows
- View ML artifacts
- Monitor models
  
[go to top](#top)

 ### Developer tools for workspace interaction

 #### Azure ML Studio

 - launch from overview page
 - https://ml.azure.com/

#### Python SDK

```shell
# 1 Install the Python SDK
pip install azure-ai-ml
# 2 Connect to the workspace
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
# 3 sample- connect to the workspace when you create a new job to train a model
from azure.ai.ml import command
# 3.1 configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    experiment_name="train-model"
)
# 3.2 connect to workspace and submit job
returned_job = ml_client.create_or_update(job)
```

> reference
- [The reference documentation on the MLClient class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient?view=azure-python)
- [The reference documentation also includes a list of the classes for all entities](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities?view=azure-python)
- [AmlCompute Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.amlcompute?view=azure-python)

#### Work with Azure CLI

```shell
# install Azure Machine Learning extension
az extension add -n ml -y
# Work with the Azure CLI,  Each command is prefixed with az ml
az ml compute create --name aml-cluster --size STANDARD_DS3_v2 --min-instances 0 --max-instances 5 --type AmlCompute --resource-group my-resource-group --workspace-name my-workspace
# YAML schemas - compute.yml
$schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json
name: aml-cluster
type: amlcompute
size: STANDARD_DS3_v2
min_instances: 0
max_instances: 5
# create compute by using yml file
az ml compute create --file compute.yml --resource-group my-resource-group --workspace-name my-workspace
```

#### Other tools

- Azure ML SDK for R
- Azure ML VS code extension

> reference
- [az ml commands](https://learn.microsoft.com/en-us/cli/azure/ml?view=azure-cli-latest)
- [az ml commands(v1)](https://learn.microsoft.com/en-us/cli/azure/ml(v1)?view=azure-cli-latest)
- [YAML schemas](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-overview?view=azureml-api-2)

[go to top](#top)

### Make data available in Azure Machine Learning

#### Types of datastores

- Authentication methods
  - Credential-based: Use a service principal, shared access signature (SAS) token or account key to authenticate access to your storage accoun
  - Identity-based:  Use your Azure Active Directory identity or managed identity
- Types of datastores
  - Azure Blob storage
  - Azure File share
  - Azure Data Lake(Gen 1)
  - Azure Data Lake(Gen 2)
- Every workspace has **four built-in datastores** (two Azure Storage blob containers, and two Azure Storage file shares)

#### URIs

- http(s): data stores publicly or privately in an Azure Blob storage or publicly available http(s) location
- wasb(s): data stores in Azure Blob Storage
  - `wasbs://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>`
- abfs(s): data stores in an Azure Data Lake Storage Gen 2
  - `abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>`
- azureml(datastore): data stores in an Azure ML datastore, A datastore is a reference to an existing storage account on Azure
  - `azureml://datastores/<datastore_name>/paths/<folder>/<file>`
- Local: `./<path>`

#### Create datastore to an Azure Blob Storage

- [Create datastores](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2&tabs=sdk-identity-based-access%2Csdk-adls-identity-access%2Csdk-azfiles-accountkey%2Csdk-adlsgen1-identity-access)


#### Create a data asset

- main types of data asset
  - file:
  - Table: points to a folder or file, and includes a schema to read as tabular data
  - When you create a data asset and point to a file or folder stored on your local device, a copy of the file or folder will be uploaded to the default datastore workspaceblobstore. You can find the file or folder in the LocalUpload folder. By uploading a copy, you'll still be able to access the data from the Azure Machine Learning workspace, even when the local device on which the data is stored is unavailable

```python
# 1 Create a URI file data asset
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
my_path = '<supported-path>'
my_data = Data(
    path=my_path,
    type=AssetTypes.URI_FILE,
    description="<description>",
    name="<name>",
    version="<version>"
)
ml_client.data.create_or_update(my_data)
# read the data from URI file data asset(points to a CSV file)
import argparse
import pandas as pd
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()
df = pd.read_csv(args.input_data)
print(df.head(10))
# 2 Create a URI folder data asset
import argparse
import glob
import pandas as pd
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()
data_path = args.input_data
all_files = glob.glob(data_path + "/*.csv")
df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)
# 3 Create a MLTable data asset
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
my_path = '<path-including-mltable-file>'
my_data = Data(
    path=my_path,
    type=AssetTypes.MLTABLE,
    description="<description>",
    name="<name>",
    version='<version>'
)
ml_client.data.create_or_update(my_data)
```

[go to top](#top)

### Automated machine learning

1. prepare data- create data asset
2. scaling and normalization to numeric data automatically, You can choose to have AutoML apply preprocessing transformations, such as:
   1. Missing value imputation to eliminate nulls in the training dataset
   2. Categorical encoding to convert categorical features to numeric indicators
   3. Dropping high-cardinality features, such as record IDs
   4. Feature engineering (for example, deriving individual date parts from DateTime features)
3. run Automated Machine Learning- configure and submit the job with the Python SDK
   1. Restrict algorithm selection(optional)
   2. Configure an AutoML experiment(Python SDK v2)
   3. Set the limits(optional) by using `set_limits()`
      - `timeout_minutes`: Number of minutes after which the complete AutoML experiment is terminated
      - `trial_timeout_minutes`: Maximum number of minutes one trial can take
      - `max_trials`: Maximum number of trials, or models that will be trained
      - `enable_early_termination`: Whether to end the experiment if the score isn't improving in the short term
   4. Set the training properties(optional)
   5. Submit an AutoML experiment


```python
# 1 create MLTable file stored in a folder
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import Input
my_training_data_input = Input(type=AssetTypes.MLTABLE, path="azureml:input-data-automl:1")
# 3.1 configure the classification job
from azure.ai.ml import automl
classification_job = automl.classification(
    compute="aml-cluster",
    experiment_name="auto-ml-class-dev",
    training_data=my_training_data_input,
    target_column_name="Diabetic",
    primary_metric="accuracy",
    n_cross_validations=5,
    enable_model_explainability=True
)
# 3.3 set limits to an AutoML experiment or job
classification_job.set_limits(
    timeout_minutes=60,
    trial_timeout_minutes=20,
    max_trials=5,
    enable_early_termination=True,
)
# 3.5 submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    classification_job
)
# 3.5 monitor AutoML job runs in the Azure Machine Learning studio
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

> reference
- [how to create a MLTable data asset in Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&tabs=cli)
- [overview of supported algorithms](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train?view=azureml-api-2#supported-algorithms?azure-portal=true)


4. Evaluate and compare models after automated ML
5. Retrieve the best run and its models
   - [Evaluate automated machine learning experiment results](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2)

[go to top](#top)

### Run a training script as a command job in Azure Machine Learning

- notebook are ideal for exploration and development, Scripts are ideal for testing and automate in production env
- to create a production-ready scipt, need to
  - remove nonessential code
  - refactor code into functions
  - test script(in the terminal)
- Run a script as a command job(To run a script as a command job, you'll need to configure and submit the job). Use the `command` function and need to specify values for the following parameters:
  - `code`: The folder that includes the script to run
  - `command`: Specifies which file to run
  - `environment`: The necessary packages to be installed on the compute before running the command
  - `compute`: the compute to use to run the command
  - `display_name`: The name of the individual job
  - `experiment_name`: The name of the experiment the job belongs to
  - [the command function and all possible parameters](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml?view=azure-python)
-

```python
from azure.ai.ml import command
# configure job
job = command(
    code="./src",
    command="python train.py --training_data diabletes.csv",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-model",
    experiment_name="train-classification-model"
    )
# submit job
returned_job = ml_client.create_or_update(job)
```

[go to top](#top)

### Optimize model training with pipelines in Azure Machine Learning(Machine Learning Operations-MLOps)

- **Pipelines** is a workflow of machine learning tasks in which each task is defined as a component, it contains steps related to the training of a machine learning model
- **Components** allow you to create reusable scripts that can easily be shared across users within the same Azure Machine Learning workspace, can use components to build an Azure ML pipeline

#### component

- A component consists of three parts:
  - Metadata: Includes the component's name, version, etc
  - Interface: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts)
  - Command, code and environment: Specifies how to run the code
- To create a component, need two files:
  - A script that contains the workflow
  - A YAML file to define the metadata, interface, and command, code, and environment of the component. Use the `command_component()` function as a decorator to create the YAML file
- [Create and run machine learning pipelines using components with the Azure Machine Learning SDK v2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2)

```python
# 1 prepares the data by removing missing values and normalizing the data
# import libraries
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler
# setup arg parser
parser = argparse.ArgumentParser()
# add arguments
parser.add_argument("--input_data", dest='input_data', type=str)
parser.add_argument("--output_data", dest='output_data', type=str)
# parse args
args = parser.parse_args()
# read the data
df = pd.read_csv(args.input_data)
# remove missing values
df = df.dropna()
# normalize the data
scaler = MinMaxScaler()
num_cols = ['feature1','feature2','feature3','feature4']
df[num_cols] = scaler.fit_transform(df[num_cols])
# save the data as a csv
output_df = df.to_csv(
    (Path(args.output_data) / "prepped-data.csv"),
    index = False
)
# 2 create a component for the prep.py script with prep.yml
# 2.1 prep.yml
$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: prep_data
display_name: Prepare training data
version: 1
type: command
inputs:
  input_data:
    type: uri_file
outputs:
  output_data:
    type: uri_file
code: ./src
environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest
command: >-
  python prep.py
  --input_data ${{inputs.input_data}}
  --output_data ${{outputs.output_data}}
# 2.2 load the component
from azure.ai.ml import load_component
parent_dir = ""
loaded_component_prep = load_component(source=parent_dir + "./prep.yml")
# 2.3 register a component
prep = ml_client.components.create_or_update(prepare_data_component)
```

#### Pipeline

- pipeline is a workflow of machine learning tasks in which each task is defined as a component
- Components can be arranged sequentially or in parallel
- Each component can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal
- A pipeline can be executed as a process by running the pipeline as a pipeline job
- Each component is executed as a child job as part of the overall pipeline job
- create a pipeline
  - using YAML file
  - using `@pipeline()`
  - ![pipeline-built](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/pipeline-built.png)
- run a pipeline job
  - configure a pipeline job: defined as a YAML file
  - run a pipeline job
  - schedule a pipline job: by using `JobSchedule` class 

[go to top](#top)

### Azure ML Models

#### Steps for training model

- create a pipeline
- add and explore a dataset
  - train regression model
  - make a prediction based on characteristics
  - azure ML has sample dataset
- apply data transformations
  - prepare data for modeling
  - address issues discovered during data exploration
  - such as select columns in dataset, clean missing data, normalize data
- execute the pipeline
- review tranformed data

#### Regression Models

- supervised ML
- Predict label based on features
- Train model with features and known values
- Use trained model to predict labels for unknown items

| Azure Regression Model| description |
|---|---|
|Bayesian Linear Regression | Linear used for small datasets|
|Boosted Decision Tree Regression | Rapid training and high accuracy, boosted -> each generated tree is based on the last|
|Decision Forest Regression | Rapid training and high accuracy|
|Fast Forest Quantile Regression | For distribution prediction|
|Linear Regression | Rapid training linear model|
|Neural Network Regression | Slow training and high accuracy, allow to create your own custom regression strategy|
|Ordinal Regression | Ranked order datasets|
|Poisson Regression | Event count prediction|

![Bayesian Linear Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Bayesian-Linear-Regression.png)
![Boosted Decision Tree Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Boosted-Decision-Tree-Regression.png)
![Decision Forest Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Decision-Forest-Regression.png)
![Fast Forest Quantile Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Fast-Forest-Quantile-Regression.png)
![Linear Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Linear-Regression.png)
![Neural Network Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Neural-Network-Regression.png)
![Ordinal Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Ordinal-Regression.png)
![Poisson Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Poisson-Regression.png)

- Primary metrics for regression evaluation
  - R Squared/Predicted R Squared,
    - easiest to describe
    - has a python library
    - adjusted R-Squared deals with overfitting
  - Mean Square Error(MSE)/Root Mean Square Error(RMSE)
    - are good for comparing performance between models
  - Mean Absolute Error(MAE)
    - are good for comparing performance between models
  - P-values for independent variables
  - Stepwise regression/best subsets regression

[go to top](#top)

#### Classification models

- supervised ML
- Predict class for items
- Train model with features and known values
- use trained model to prdict labels for unknown items

| Azure Classification Model| description |
|---|---|
|Artificial Neural Network|Naive Bayes|
|Decision tree|Random Forest|
|K-Nearest Neighbor(KNN)|Stochastic Gradient Descent(SGD)随机梯度下降|
|Logistic Regression| Support Vector Machine(SVM)|

![Artificial Neural Network](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Artificial-Neural-Network.png)
![Decision tree](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Decision-tree.png)
![KNN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN.png)
![Logistic Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Logistic-Regression.png)
![Naive Bayes](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Naive-Bayes.png)
![Random Forest](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Random-Forest.png)
![SGD](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/SGD.png)
![SVM](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/SVM.png)

- Possible outcomes of Classification Prediction
  - true positive: prediction and the acural agree on the positive result
  - false positive: prediction is at positive, but the actual data suggests that it should have been negative
  - true negative: prediction and the acural agree on the negative result
  - false negative: prediction is at negative, but the actual data suggests that it should have been positive
- Primary metrics for regression evaluation
  - Accuracy
  - Precision
  - Recall:  the ratio of `true positives/(true negatives + false negatives)`, what the cost of a false negative is.
- Commnon evaluation metrics
  - Logarithmic loss
  - Confusion matrix
  - Area under curce(AUC)
  - F1 score
  - Mean absolute error(MAE)
  - Mean squared error(MSE)
  - Gain and lift charts
  - Kolmogorov-Smirnov charts
  - Log loss
  - Gini coefficient
  - Root Mean squared Error(RMSE)
  - Concordant-discordant ratio

|pipeline of Classification regression| details |pipeline of Classification evaluation| details |
|---|---|---|---|
|sample-pipeline-regression| ![sample-pipeline-regression.png](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-pipeline-regression.png)| sample-evaluation-regression |![sample-evaluation-regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-evaluation-regression.png)|
|sample-pipeline-classification|![sample-pipeline-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-pipeline-classification.png)|sample-evaluation-classification| ![sample-evaluation-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-evaluation-classification.png)|
|sample-inference-classification|![sample-inference-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-inference-classification.png)| | |

[go to top](#top)

#### Clustering models

- unsupervised learning
- useful for identifying partterns
- 聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)。通过这样的划分每个簇可能对应于一些潜在的概念，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名
- types of clustering Algorithm
  - Centroid
  - Density
  - Distribution
  - Hierarchical

|Azure Clustering Model| description |Azure Clustering Model| description |
|---|---|---|---|
|Affinity Propagation|![https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Affinity Propagation](Affinity-Propagation.png) |Agglomerative|![Agglomerative](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Agglomerative.png) |
|BIRCH(Balanced Iterative Reducing and Clustering using Hierachies Model)|![BIRCH](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/BIRCH.png)|DBSCAN(Density-Based Spatial Clustering of Applications with Noise)|![DBSCAN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/DBSCAN.png)|
|K-Means|![K-Means](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/K-Means-cluster.png)|Mean Shift|![Mean Shift](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Mean-Shift.png)|
|OPTICS(Ordering Points to Identify the Clustering Struture)|![OPTICS](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/OPTICS.png)|Caussian Mixture|![Caussian Mixture](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Caussian-Mixture.png)|

- steps for Clustering Mode Training
  - create dataset
  - create pipeline
  - apply transformations
  - run pipeline
  - review tranformed data
- ![pipeline for Clustering Model](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/pipeline-for-Clustering-Model.png) ![evaluation for Clustering Model](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/evaluation-for-Clustering-Model.png)

> references
- [Visualizing K-Means Clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
- [Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)
- [刘建平Pinard-K-Means聚类算法原理](https://www.cnblogs.com/pinard/p/6164214.html)

[go to top](#top)

### Project Jupyter & Notebooks

- Jupyter projecy
  - IPython(Julia, Python, RProducts) project
  - perform ML, Deep learning activities
  - perform data cleansing
  - Statistical modeling
  - Data Visualization
- Jupyter projecy products
  - Jupyter client
  - Jupyter notebook
  - Jupyter Kernels
  - IPykernel package
- Jupter tools
  - JupyterLab
  - nbconvert
  - nbviewer
  - Otconsole
- Jupyter Notebook
  - Web application: client-server App, saved as **ipynb** file
  - document collaboration: Equations, live code, narrative text, visualization
  - Rich text and graphics
  - notebook file(**xxx.ipynb**)
    - Metadata
    - versions
    - cell content
      - cell types
      - Headers
      - text styling
      - lists
      - syntax highlighting

|Kernel| Language |
|---|---|
| IJavascript|Javascript |
|IJulia|Julia|
|IHaskell|Haskell|
|IPHP|PHP|
|IRKernel|R|
|IRuby|Ruby|

|compute type| Description |
|---|---|
|Compute instance|notebooks|
|compute clusters|pipeline|
|inference clusters|inference pipeline|

- PyTorch: 既可以看作加入了GPU支持的numpy，同时也可以看成一个拥有自动求导功能的强大的深度神经网络
  - 已经被Facebook, Twitter、CMU和Salesforce等机构采用
- TensorFlow: 是谷歌基于DistBelief进行研发的第二代人工智能学习系统
  - Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端的计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。
 TensorFlow可被用于语音识别和图像识别等多项机器学习和深度学习领域，对2011年开发的深度学习基础架构DistBelief进行了各方面的改进，它可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行。
  - 支持CNN(卷积神经网络)、RNN(循环神经网络)和LSTM(长短期记忆网络)算法，是目前在 Image，NLP最流行的深度神经网络模型

[go to top](#top)

### Azure Data Platform Services

```
                                            Azure Data Platform Services
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Managed relational DB                   |  requirement for on-demanded scaling, no downtime, intelligent optimization
  Azure SQL DB    | Structured and unstructured             |  focus on security and availability
                  | Online Transaction Processing(OLTP)     |  Reduced costs associated with on-site infrastructure
                  | Strong security and availability        |  protected by SLA(Azure service Level Agreement)
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Globally distributed                    |  requirement for NoSQL
                  | Multimodel                              |  focus on global scale, regional and automatic failover
Azure Cosmos DB   | API models                              |  need for low latency
                  |  - SQL API, MongoDB API, Gremin API,    |  <1 second response time, 10ms read/write guarantee
                  |  - Table API, Cassandra API             |  strong consistency levels
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Azure base storage type                 |  need to store but not query data
                  |   -Blob, files, queue, table            |  need to work with unstructured data and images
Azure Data Storage| scalable object store                   |  require a cost-effective solutions
                  | Messaging store                         |  scalable and highly available, no downtime
                  | NoSQL store                             |  Azure-managed hardware maintenance, REST API and SDKs
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Hadoop-compatiable                      |  big data analytics
                  |   -Blob, files, queue, table            |  Hierarchal filesystem
Azure Data Lake   | scalable object store                   |  Azure blob storage
Storage Gen2      | Messaging store                         |  scalable and highly available, no downtime
                  | NoSQL store                             |  Azure-managed hardware maintenance, REST API and SDKs
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Data warehousing                        |  unified management, monitoring, security
Azure Synapse     | big data analytics                      |  Massively parallel processing(MPP)
Analytics         | Unlimited scaling                       |  Synapse SQL, Apache Spark for Azure Synapse
                  | Strong security and availability        |  Data Lake functionality,
                  |                                         |  Synapse Studio
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Internet of Things(IoT)                 |  Real-time response, continuous data monitoring
Azure Stream      | web logs                                |  streaming or batch data
Analytics         | Ongoing monitoring                      |  Data Lake functionality,
                  | Point of Sale(POS)                      |  Synapse Studio
                  | Extremely cost effective                |
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | data warehousing tool                   |  Azure Databricks Integration
                  | Apache Spark capable                    |    - Azure Synapse analytics, Azure Data Lake storage
Azure Databricks  | managed Spark clusters                  |    - Azure CosmosDB, Azure Blob storage, Power BI
                  | managed SQL endpoints,Alerts,Dashboards |
                  | Azure databricks notebooks              |
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | cloud-based ETL and integration platform|  Use Azure HDInsight, Spark, Hadoop, Azure ML
Azure Data Factory| Control data movement between stores    |  Public to Azure SQL data warehouse
                  | managed Spark clusters                  |  Organize raw data
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | wrapped Hadoop, HBase, Storm, Kafka     |  Cloud native, Scalability and cost, Monitoring, Productivity, Extensibility,
Azure HDInsight   | Control data movement between stores    |  Security and compliance, Global availability
                  | managed Spark clusters                  |
------------------|-----------------------------------------|----------------------------------------------------------------------------
```
- Azure Data Lake Gen2 Features
  - Unlimited scaling
  - Hadoop-compatible
  - Access control list(ACL) support
  - Zone-redundant storage
  - Geo-redundant storage
  - POSIX compliant
  - Optimized Azure Blob File System(ABFS) driver
- Azure Databricks
  - Azure Databricks Workspace
    - Spark SQL and dataFrames
    - Streaming
    - MLib
    - GraphX
    - Spark Core API
  - Managing Spark Clusters
    - Quickly create clusters
    - Autoscaling up and down
    - Share clusters across teams
    - Program clusters using REST APIs
    - Secure data integration
    - No need to centralize unified data
    - Automatic Spark updating
- Azure Data Factory
  - How Azure Data Factory works
    - Connect and collect
    - Transform and enhance
    - CI/CD and publish
    - Monitor
  - Azure Data Factory components
    - pipelines, Activities, Datasets, linked services, Data flows, Integration runtimes

#### Security of Data Platform Services

|DB| Security |DB| Security |
|---|---|---|---|
| |Advanced Threat Protection(ATP)| | Automatic data encryption|
| |Strong encryption| |IP firewall|
|Azure SQL DB|SQL DB auditing|Azure Cosmos DB| Virtual networks|
| |Multi-factor authentication(MFA)| | Token-based user authentication|
| |Azure Active Directory(ADD) authentication| |Role-based security with ADD|
| |Compliance certification| |Compliance certification|
|------------------|------------------------------------|------------------|------------------------------------|
| |Data encryption | |Automatic encryption at rest|
| |Precise access control | |Active Directory(AD) security groups |
|Azure data storage |Keys or shared access signatures|Azure Data Lake Gen2 |Role-based access control(RBAC)|
| |Azure Resource Manager| |Built-in security groups |
| |Role-based access control(RBAC)| |firewall |
|------------------|-----------------------------------|------------------|------------------------------------|
| |Azure Active Directory(ADD) | |Transport layer between device and IoT hub |
| |Multi-factor authentication | |Shared key to secure transfer|
|Azure Synapse Analytics | Column-based security |Azure Stream analytics |Streaming data ignored |
| |Row-based security | | Data security provided by storage device |
| |SQL Server authentication |  |   |
|------------------|-----------------------------------|------------------|------------------------------------|
| |Azure active Directory | |Transport layer between device and IoT hub |
| |Role-based access | |Shared key to secure transfer|
|Azure databricks |enterprise-grade service level agreements(SLAs)|Streaming data ignored |
| |Column-based security | | Data security provided by storage device |
| |Row-based security| |  |
|------------------|-----------------------------------|------------------|------------------------------------|
| |Encryption | |  |
|Azure HDInsight |Secure Shell(SSH) | |  |
| |Shared access signatures| | |
| |Azure Active Directory(AAD) | |  |

#### Working with Data in Data Platform Services

|Working with| description |Working with| description |
|---|---|---|---|
| |Software development Kits(SDKs) | | Azure Data Factory|
|Azure SQL DB |.Net, Python, Java, NodeJS|Azure Cosmos DB |Azure Cosmos DB API|
| |Transact-SQL(T-SQL) | |JSON|
| | Azure Data Factory| | Direct editing|
|------------------|------------------------------------|------------------|------------------------------------|
| |AzCopy | | Apache sqoop|
| |Azure Data Factory |  |AzCopy|
|Azure Data storage |Powershell |Azure data lake storage Gen2 |Azure Data Factory|
| |Storage explorer | | Azure storage explorer|
| |Visual studio | | Powershell|
| | | |Visual studio|
|------------------|------------------------------------|------------------|------------------------------------|
| |Extract, load, transform(ELT) | |Azure Event Hubs |
|Azure Synapse Analytics |PolyBase |   |Azure IoT hub|
| |Azure Data Factory | |Azure Blob storage |
|------------------|------------------------------------|Azure Stream analytics |Inut and output pipelines |
|Azure data factory |Hive for ETL operations | |Route job output |
| |Azure data factory for Hive queries | |Batch analytics |
| | | |Consume with other service |
| | | |Send to Power BI |
|------------------|------------------------------------|------------------|------------------------------------|
| |Java and Python | | |
|Azure HDInsight |Mapper, Reducer | | |
| |Spark, Anaconda libraries | | |
| |GraphX, Storm | |  |

[go to top](#top)

### Azure Storage Accounts

**Storage acount setting**

| setting|Description|
|---|---|
| Subscription  |Billing account  |
|location| Data center|
|Performance|Data services and hard disks|
|Replication|Backup|
|Access tier| Determines speed of blob access|
|Secure transfer required| HTTP or HTTPS|
|Virtual networks|Limit inbound access to virtual networks|

- Account type
  - storageV2
  - storage
  - Blob storage

#### Azure Blob storage

- cloud object storage
- Massive unstructured data

```
-------------|---------- uses -----------------------------|-------- Accessing Object -----------------------
             | serving files to browsers                   |  HTTP/HTTPS
             | storage for distributed access              |  Azure storage REST API
             | streaming audio and video                   |  Azure PowerShell
Blob storage | streaming audio and video                   |  Azure CLI
             | Log file writing                            |  Azure storage client library
             | Backup, disaster recoverty, archiving       |
             | analysis by on-site or Azure-hosted service |
-------------|---------------------------------------------|-------------------------------------------------
```

- Azure Blob storage and Azure Data Lake storage Gen2
  - Tierd storage
  - High availability
  - Consistent
  - Disaster recovery
- Azure Blob resources
  - Storage accounts
    - unique namespace
    - objects include account name
    - object base address
  - Containers
    - all blobs must be stored in a blob container
    - used to organize blobs
    - unlimited containers for storage accounts
    - unlimited blobs for containers
  - Blobs types
    - Block blobs
    - append blobs
    - page blobs
  - Moving data to blob storage
    - AzCopy
    - Azure Data factory
    - Azure Data Box
    - Azure import/export service
    - Azure storage Date movement library
    - Blobfuse

#### Azure File Shares

- features
  - Fully managed shares
  - Mounted concurrently
  - Serve Message Block(SMB)
  - Networkd File System(NFS)
- uses
  - replacing or complimenting on-site hardware
  - lifting and shifting apps
  - containerizing
  - simplifying development
- Options
  - Premium, Transaction-optimized, Hot, Cool
- Azure fiel shares and cloud development
  - development, testing, debugging
  - shared applications
  - diagnostics
- soft delete: need to take snapshots periodically

#### Azure Table storgae

- feature
  - Structured NoSQL data
  - key/attribute store
  - Schema-free design
- Table storage
  - store and query massive amounts of data
  - tables scale with demand
  - flexible dataset storage
  - unlimited number of enitites in a table
  - unlimited number of tables in storage accounts
- Azure Table storage components
  - URL format
  - Account
  - Table
  - Entity
  - Properties
- Common User cases for Azure table storage
  - serving web application
  - fast queries with clustered index
  - Datasets that can be denormalized
  - Using OData and LINQ queries wiht WCF data service .NET libraries

#### Azure Queue storgae

- features
  - large amount of **messages**
  - messages can be accessed from anywhere
  - HTTP or HTTPS
  - Messages up to 64KB
- Azure Queue storage components
  - URL format
  - Storage account
  - Queue
  - Message
- Advantages
  - Large workloads
  - Client libraries
  - REST API access
  - decoupling components
  - Resilient application development
  - Burst scaling

[go to top](#top)

### Azure Storage Strategy

- determining Data and performance requirement
  - simple lookups
  - database queries
  - creation, updating, deletion activities
  - complex data analysis
  - speed of data operations
- Transactions
  - group of operations
  - group execution
  - ACID(Atomicity, Consistency, Isolation, Durability)
  - OLTP or OLAP
- Azure Core Storage Services
  - Azure blobs
  - Azure files
  - Azure Queues
  - Azure Tables
  - Azure Disks
- Encryption for core storage services
  - At rest
  - intraction(Client-side)

|Data type|Structure |Service|
|---|---|---|
|Product catalogs |Semi-structured|Azure Comsmos DB|
|Video and photographs|Unstructured|Azure Blob storage|
|Business data|Strutured| Azure SQL Database|


####  Data structure type

```
--------------------|--------------------------------------|------------------------------------
                    | relational model                     | slow when changes made
                    | Table structure                      | bulk update for new columns
 Structured data    | Column width                         | use query language
                    | Defined at time of design            |
                    | Designed before data is populated    |
--------------------|--------------------------------------|------------------------------------
                    | Binary, audio, image files           | key-value store
                    | No-relational(NoSQL)                 | document database
 Unstructured data  | Raw data format                      | graph database
                    | not defined at time of design        | column database
                    | structure defined when data read     |
--------------------|--------------------------------------|------------------------------------
                    | Less organized than structured data  |
Semi-structured data| organization through tags            |
                    | handled by No-relational(NoSQL) DB   |
--------------------|--------------------------------------|------------------------------------
```

#### Azure storage security

- Advantages
  - Encrpytion at rest
  - Encrpytion in transit
  - User access control
  - Browser cross-domain access
  - Storage access auditing
- Blob and Queue storage security
  - Active Directory(AD) authorization
  - Access control for blobs and Queue storags
- Storage account keys
  - primary and secondary key
  - Blobs, files, queues, tables
  - Easy to implement
  - HTTP authorization header
  - Protecting Shared Keys
- Shared Access Signatures(SAS)
  - Untrusted clients
  - control storage object access
  - Security token attached to URI
  - Types of SAS
    - Service-level SAS
    - Account-level SAS
- Controlling storage acount network access
  - manage default rules
  - ensure necessary services have access
- Azure Storage Advanced Threat Protection(ATP)
  - Unexpected behavior detection
  - Security alerts
  - Azure Defender for Storage(Analyse tool)
  - Hierarchal namespaces

#### Console application demo by using Visual Studio Code

```shell
# open new terminal in Visual Studio Code
donet new console   #create new console application
mkdir data          #create new directory to story data
donet add package Azure.Storage.Blobs
#edit program.cs
using System;
using System.Threading.Tasks;
using Azure.Storage.Blobs;

namespace sbdemo10_05
{
  class program
  {
    static async Tak Main(string[] args)
    {
      var connectionString = "xxxxxxxxxxxxxxxxxxxxxxxx";
      var container = new BlobContainerClient(connectionString, "demo");
      await container.CreateIfNotExistsAsync();
      # create a new file demo.txt in data directory
      var fileName = "demo.txt";
      var blob = container.GetBlobClient(fileName);
      await blob.UploadAsync(Path.combine("data", fileName), true);
      await blob.DownloadAsync(Path.combine("download", fileName));
    }
  }
}
# back to terminal in Visual Studio Code
dotnet run
```

#### Azure Storage Services

- Features
  - Managed
  - Scalable
  - Secure
  - Durable
- Azure data Types
  - Blobs
  - Files
  - Queues
  - Tables storage
- Azure storage Limits
  - 200 storage accounts
  - 500 TB per account
- Azure storage accounts
  - General-purpose v2(GPv2)
  - General-purpose v1(GPv1)
  - Blob storage accounts
- Azure Disk storage types
  - Managed
  - Unmanaged
- Azure Disk Types
  - Standard
  - Premium

[go to top](#top)

### Azure Data Factory

- SQL server
- Data management gateway
- Azure Storage
- Azure SQL database
- ETL(Extract, Transform, Load), ELT(Extract, Load, Transform)
- Data-driven workflows, pipeline, dataset, activity
- Mapping data flow, control flow, trigger
- Linked service, connect
- Transform and enhance
- Continuous Integration(CI) and Continuous Delivery(CD)
- Monitoring
-
#### Azure Data Factory Elements

- Activity: step in the workflow
- Linked service:
- Integartion Runtime(IR): provides the bridge between the activity and Linked service

#### Azure Data Factory Integration Runtime

- IR is the underlying compute infrastructure that data factory uses to execute workflows across different networks
  - data flow
  - data movement
  - Activity dispatch: executing tranformation activities on various other services like databricks, HDInsight, ML
  - SSIS package execution
- how to using
  - Azure Data Factory UX
  - Management hub
- Types of IR
  - Azure
  - Self-hosted: cannot perform data flow
  - Azure SQL Server Integation Services(SSIS)
- Azure IR activities
  - Execute data flows using Azure
  - Execute copy activity between data stores
  - Apply transform activities
- Azure IR Networking
  - connect to data stores and computing service
  - connect with private link using Managed Virtual Network
- Azure IR Resources and scaling
  - managed and serverless
  - secure, high performance data movement
  - Lightweight routing with activity dispatch, it is no need to scale compute
- Azure IR Linked services and datasets
  - ![Azure IR Linked services](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-IR-Linked-services.png)

#### Azure Data Factory Triggers

- types
  - manual: powershell, .NET SDK, REST API, Python SDK
  - scheduled
  - event-driven:
- pipelines and triggers
  - many-to-many relationship
  - multiple triggers can execute single pipeline
  - single trigger can execute multiple pipeline
- JSON properties for scheduling triggers
  - startTime, endTime, timeZone
  - recurrence, frequency, interval, schedule

[back to top](#top)

### Non-relational Data Stores

 - Types
   - Columnar
   - Document
   - External index
   - Graph
   - Key/value
   - Object
   - Time series
 - Unstructured Data formats
   - CSV(Comma-separated values)
   - JSON(Javascript Object Notation)

#### Azure Cosmos DB

  - NoSQL DB
  - Globally distributed,
    - regional failover, automatic failover, Multmaster replication
  - Multimodel
  - 99.999 percent uptime
  - Low latency
  - response time under 10ms
 - working with Azure Cosmos DB
   - Data ingestion
   - Data security
   - Queries
 - Azure Cosmos DB API Models
   - SQL API
   - MongoDB API
   - Gremlin API: graph API
   - Table API
   - Cassandra API
- Azure Cosmos DB schema and JSON
 - Schema-free
 - Automatic indexing
 - come in and out respented as JSON documents(node) and documents converted to trees
- Azure Cosmos DB Index types
  - range: great in tree structure, used for scalar values like strings and numbers
  - spatial: allow to search for data based on mathematical relationship to other data, such as distance and intersect
  - composite

#### Azure Blob storage

- Features
  - serving images and documents to a browser
  - audio and video streaming
  - distributed access
  - writing to log files
  - backup, restore, archiving
  - data analysis
- Access to blob storage
  - HTTP(S)
  - Azure Storage REST API
  - Azure Powershell
  - Azure CLI
  - Azure storage client library: .NET, JAVA, Nodejs, Python, Go, PHP, Ruby
- Azure Blob storage resource
  - Storage account
  - container
  - Blob
- Azure Blob storage security
  - Data protection
  - Identity and access management(IAM)
  - Networking, such as firewall
  - Monitoring and logging

#### Azure Data Lake Storage Gen2

- Big data analytis -> Massive amounts of data, massive throughput
- Hierarchal namespaces
- Azure blob storage
- work with Azure Data Lake Storage Gen2
  - Ingesting
  - processing
  - downloading
  - visualizing
- Common data types
  - ad hoc
  - azure HDInsight clusters
  - On-sites or IaaS Hadoop clusters
  - Relational
  - streamed
  - Very large datasets
  - Web server logs
- Processing Data
  - Azure HDInsight
  - Azure Databricks
- Visualing Data
  - Power BI connector
  - Generate visual representations
- Downloading Data
  - Azure data factory
  - Azure storage explorer
  - Apache DistCp
  - AzCopy

#### Azure Data Masking

- Azure SQL database, Azure SQL Managed Instance, Azure Synapse Analytics
- Dynamic data masking
  - Azure web portal through Dynamic data masking blade
  - powerdshell or REST API
- Data Masking policy
  - SQL users excluded from masking
  - masking rules
  - masking functions

#### Azure Data Encryption at Rest

- Encryption Models
  - Client-side
  - Server-side
  - Disk
  - storage service

[back to top](#top)

> references
- [Microsoft ML Learn - Hands On Exercises](https://microsoftlearning.github.io/mslearn-azure-ml/)
- [Azure Machine Learning examples](https://learn.microsoft.com/en-us/samples/azure/azureml-examples/azure-machine-learning-examples/)
