## [Designing and Implementing a Data Science Solution on Azure(DP-100) - Azure Data Scientist Associate](#top)

- [Azure ML workspace](#azure-ml-workspace)
  - [Access to Azure Machine learning workspace](#access-to-azure-machine-learning-workspace)
  - [Azure ML resources](#azure-ml-resources)
  - [Azure ML assets](#azure-ml-assets)
  - [Train Models in the workspace](#train-models-in-the-workspace)
- [Developer tools for workspace interaction](#developer-tools-for-workspace-interaction)
  - [Azure ML Studio](#azure-ml-studio)
  - [Python SDK](#python-sdk)
  - [Work with Azure CLI](#work-with-azure-cli)
- [Make data available in Azure Machine Learning](#make-data-available-in-azure-machine-learning)
   - [Types of datastores](#types-of-datastores)
  - [URIs](#uris)
  - [Create datastore to an Azure Blob Storage](#create-datastore-to-an-azure-blob-storage)
  - [Create a data asset](#create-a-data-asset)
- [Automated machine learning](#automated-machine-learning)
- [Run a training script as a command job in Azure Machine Learning](#run-a-training-script-as-a-command-job-in-azure-machine-learning)
- [Optimize model training with pipelines in Azure Machine Learning(Machine Learning Operations-MLOps)](#optimize-model-training-with-pipelines-in-azure-machine-learningmachine-learning-operations-mlops)
  - [component](#component)
  - [Pipeline](#pipeline)
- [Azure ML Models](#azure-ml-models)
  - [Steps for training model](#steps-for-training-model)
  - [Regression Models](#regression-models)
  - [Classification models](#classification-models)
  - [Clustering models](#clustering-models)
- [Project Jupyter \& Notebooks](#project-jupyter--notebooks)

### Azure ML workspace

![Azure-ML-workspace1](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace1.png)
![Azure-ML-workspace2](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace2.png)

#### Access to Azure Machine learning workspace

- Three general built-in roles
  - Owner: full access to all resources, can grant access to others
  - Contributor: full access to all resources, can't grant access to others
  - Reader: only view the resources
- Azure machine learning's specific built-in roles
  - AzureML Data Scientist
  - AzureML compute Operator
- to fully customize permissions, create a custom role

#### Azure ML resources

- Workspace:  top-level for ML, stores all logs, metrics, outputs, models and snapshots
- Compute resources:
  - compute instance: similar to VM in the cloud, managed by workspace
  - compute cluster: on-demand clusters of CPU or FPU compute nodes in the cloud
  - inference clutster: allows to create or attach Azue Kubernetes Service(AKS) cluster Idela to deploy trained ML models in production scenarios
  - attached compute: allow to attach other Azure compute resources suck as Databricks or Synapse Spark pools
- Datastores: all data is stored in datastores which references to azure data services. When connected to the workspace, two datastores will added to your workspace: **workspaceFilestore** and **workspaceblobstore**

#### Azure ML assets

- Models:
- Environments: stored as an image in Azure Container Registry created with workspace when it's used for the first time
- Data: can use data assets to easily access data every time, without having to provide authentication every time. When create a data assets in the workspace, you'll specify the path to point to the file or folder, name and version.
- Components: make it easier to share code with component in a workspace

#### Train Models in the workspace

- Author and run a pipeline with the designer
- Automated ML: explore algorithms and hyperparameter values
- Jupyter notebook:
  - files:  will stored in the file share of storage account
  - run notebook: use a compute instance
  - can edit and run notebook in Visual Studio Code
- Running a pipeline with the designer
- Run a script as a job
  - command
  - Sweep: perform hyperparameter tuning when executing a single script
  - Pipeline: rung a pipeline consisting of multiple scripts or components

[go to top](#top)

 ### Developer tools for workspace interaction

 #### Azure ML Studio

 - launch from overview page
 - https://ml.azure.com/

#### Python SDK

```shell
# 1 Install the Python SDK
pip install azure-ai-ml
# 2 Connect to the workspace
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
# 3 sample- connect to the workspace when you create a new job to train a model
from azure.ai.ml import command
# 3.1 configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    experiment_name="train-model"
)
# 3.2 connect to workspace and submit job
returned_job = ml_client.create_or_update(job)
```

> reference
- [The reference documentation on the MLClient class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient?view=azure-python)
- [The reference documentation also includes a list of the classes for all entities](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities?view=azure-python)
- [AmlCompute Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.amlcompute?view=azure-python)

#### Work with Azure CLI

```shell
# install Azure Machine Learning extension
az extension add -n ml -y
# Work with the Azure CLI,  Each command is prefixed with az ml
az ml compute create --name aml-cluster --size STANDARD_DS3_v2 --min-instances 0 --max-instances 5 --type AmlCompute --resource-group my-resource-group --workspace-name my-workspace
# YAML schemas - compute.yml
$schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json
name: aml-cluster
type: amlcompute
size: STANDARD_DS3_v2
min_instances: 0
max_instances: 5
# create compute by using yml file
az ml compute create --file compute.yml --resource-group my-resource-group --workspace-name my-workspace
```

> reference
- [az ml commands](https://learn.microsoft.com/en-us/cli/azure/ml?view=azure-cli-latest)
- [az ml commands(v1)](https://learn.microsoft.com/en-us/cli/azure/ml(v1)?view=azure-cli-latest)
- [YAML schemas](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-overview?view=azureml-api-2)

[go to top](#top)

### Make data available in Azure Machine Learning

#### Types of datastores

- Authentication methods
  - Credential-based: Use a service principal, shared access signature (SAS) token or account key to authenticate access to your storage accoun
  - Identity-based:  Use your Azure Active Directory identity or managed identity
- Types of datastores
  - Azure Blob storage
  - Azure File share
  - Azure Data Lake(Gen 1)
  - Azure Data Lake(Gen 2)
- Every workspace has **four built-in datastores** (two Azure Storage blob containers, and two Azure Storage file shares)

#### URIs

- http(s): data stores publicly or privately in an Azure Blob storage or publicly available http(s) location
- wasb(s): data stores in Azure Blob Storage
  - `wasbs://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>`
- abfs(s): data stores in an Azure Data Lake Storage Gen 2
  - `abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>`
- azureml(datastore): data stores in an Azure ML datastore, A datastore is a reference to an existing storage account on Azure
  - `azureml://datastores/<datastore_name>/paths/<folder>/<file>`
- Local: `./<path>`

#### Create datastore to an Azure Blob Storage

- [Create datastores](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2&tabs=sdk-identity-based-access%2Csdk-adls-identity-access%2Csdk-azfiles-accountkey%2Csdk-adlsgen1-identity-access)


#### Create a data asset

- main types of data asset
  - URI file:
    - When you create a data asset and point to a file or folder stored on your local device, a copy of the file or folder will be uploaded to the default datastore workspaceblobstore. You can find the file or folder in the LocalUpload folder. By uploading a copy, you'll still be able to access the data from the Azure Machine Learning workspace, even when the local device on which the data is stored is unavailable
  - URI folder
  - MLTable: points to a folder or file, and includes a schema to read as tabular data

```python
# 1 Create a URI file data asset
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
my_path = '<supported-path>'
my_data = Data(
    path=my_path,
    type=AssetTypes.URI_FILE,
    description="<description>",
    name="<name>",
    version="<version>"
)
ml_client.data.create_or_update(my_data)
# read the data from URI file data asset(points to a CSV file)
import argparse
import pandas as pd
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()
df = pd.read_csv(args.input_data)
print(df.head(10))
# 2 Create a URI folder data asset
import argparse
import glob
import pandas as pd
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()
data_path = args.input_data
all_files = glob.glob(data_path + "/*.csv")
df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)
# 3 Create a MLTable data asset
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
my_path = '<path-including-mltable-file>'
my_data = Data(
    path=my_path,
    type=AssetTypes.MLTABLE,
    description="<description>",
    name="<name>",
    version='<version>'
)
ml_client.data.create_or_update(my_data)
```

[go to top](#top)

### Automated machine learning

1. prepare data- create data asset
2. scaling and normalization to numeric data automatically, You can choose to have AutoML apply preprocessing transformations, such as:
   1. Missing value imputation to eliminate nulls in the training dataset
   2. Categorical encoding to convert categorical features to numeric indicators
   3. Dropping high-cardinality features, such as record IDs
   4. Feature engineering (for example, deriving individual date parts from DateTime features)
3. run Automated Machine Learning- configure and submit the job with the Python SDK
   1. Restrict algorithm selection(optional)
   2. Configure an AutoML experiment(Python SDK v2)
   3. Set the limits(optional) by using `set_limits()`
      - `timeout_minutes`: Number of minutes after which the complete AutoML experiment is terminated
      - `trial_timeout_minutes`: Maximum number of minutes one trial can take
      - `max_trials`: Maximum number of trials, or models that will be trained
      - `enable_early_termination`: Whether to end the experiment if the score isn't improving in the short term
   4. Set the training properties(optional)
   5. Submit an AutoML experiment


```python
# 1 create MLTable file stored in a folder
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import Input
my_training_data_input = Input(type=AssetTypes.MLTABLE, path="azureml:input-data-automl:1")
# 3.1 configure the classification job
from azure.ai.ml import automl
classification_job = automl.classification(
    compute="aml-cluster",
    experiment_name="auto-ml-class-dev",
    training_data=my_training_data_input,
    target_column_name="Diabetic",
    primary_metric="accuracy",
    n_cross_validations=5,
    enable_model_explainability=True
)
# 3.3 set limits to an AutoML experiment or job
classification_job.set_limits(
    timeout_minutes=60,
    trial_timeout_minutes=20,
    max_trials=5,
    enable_early_termination=True,
)
# 3.5 submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    classification_job
)
# 3.5 monitor AutoML job runs in the Azure Machine Learning studio
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

> reference
- [how to create a MLTable data asset in Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&tabs=cli)
- [overview of supported algorithms](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train?view=azureml-api-2#supported-algorithms?azure-portal=true)


4. Evaluate and compare models after automated ML
5. Retrieve the best run and its models
   - [Evaluate automated machine learning experiment results](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2)

[go to top](#top)

### Run a training script as a command job in Azure Machine Learning

- notebook are ideal for exploration and development, Scripts are ideal for testing and automate in production env
- to create a production-ready scipt, need to
  - remove nonessential code
  - refactor code into functions
  - test script(in the terminal)
- Run a script as a command job(To run a script as a command job, you'll need to configure and submit the job). Use the `command` function and need to specify values for the following parameters:
  - `code`: The folder that includes the script to run
  - `command`: Specifies which file to run
  - `environment`: The necessary packages to be installed on the compute before running the command
  - `compute`: the compute to use to run the command
  - `display_name`: The name of the individual job
  - `experiment_name`: The name of the experiment the job belongs to
  - [the command function and all possible parameters](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml?view=azure-python)
-

```python
from azure.ai.ml import command
# configure job
job = command(
    code="./src",
    command="python train.py --training_data diabletes.csv",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-model",
    experiment_name="train-classification-model"
    )
# submit job
returned_job = ml_client.create_or_update(job)
```

[go to top](#top)

### Optimize model training with pipelines in Azure Machine Learning(Machine Learning Operations-MLOps)

- **Pipelines** is a workflow of machine learning tasks in which each task is defined as a component, it contains steps related to the training of a machine learning model
- **Components** allow you to create reusable scripts that can easily be shared across users within the same Azure Machine Learning workspace, can use components to build an Azure ML pipeline

#### component

- A component consists of three parts:
  - Metadata: Includes the component's name, version, etc
  - Interface: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts)
  - Command, code and environment: Specifies how to run the code
- To create a component, need two files:
  - A script that contains the workflow
  - A YAML file to define the metadata, interface, and command, code, and environment of the component. Use the `command_component()` function as a decorator to create the YAML file
- [Create and run machine learning pipelines using components with the Azure Machine Learning SDK v2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2)

```python
# 1 prepares the data by removing missing values and normalizing the data
# import libraries
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler
# setup arg parser
parser = argparse.ArgumentParser()
# add arguments
parser.add_argument("--input_data", dest='input_data', type=str)
parser.add_argument("--output_data", dest='output_data', type=str)
# parse args
args = parser.parse_args()
# read the data
df = pd.read_csv(args.input_data)
# remove missing values
df = df.dropna()
# normalize the data
scaler = MinMaxScaler()
num_cols = ['feature1','feature2','feature3','feature4']
df[num_cols] = scaler.fit_transform(df[num_cols])
# save the data as a csv
output_df = df.to_csv(
    (Path(args.output_data) / "prepped-data.csv"),
    index = False
)
# 2 create a component for the prep.py script with prep.yml
# 2.1 prep.yml
$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: prep_data
display_name: Prepare training data
version: 1
type: command
inputs:
  input_data:
    type: uri_file
outputs:
  output_data:
    type: uri_file
code: ./src
environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest
command: >-
  python prep.py
  --input_data ${{inputs.input_data}}
  --output_data ${{outputs.output_data}}
# 2.2 load the component
from azure.ai.ml import load_component
parent_dir = ""
loaded_component_prep = load_component(source=parent_dir + "./prep.yml")
# 2.3 register a component
prep = ml_client.components.create_or_update(prepare_data_component)
```

#### Pipeline

- pipeline is a workflow of machine learning tasks in which each task is defined as a component
- Components can be arranged sequentially or in parallel
- Each component can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal
- A pipeline can be executed as a process by running the pipeline as a pipeline job
- Each component is executed as a child job as part of the overall pipeline job
- create a pipeline
  - using YAML file
  - using `@pipeline()`
  - ![pipeline-built](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/pipeline-built.png)
- run a pipeline job
  - configure a pipeline job: defined as a YAML file
  - run a pipeline job
  - schedule a pipline job: by using `JobSchedule` class 

[go to top](#top)

### Azure ML Models

#### Steps for training model

- create a pipeline
- add and explore a dataset
  - train regression model
  - make a prediction based on characteristics
  - azure ML has sample dataset
- apply data transformations
  - prepare data for modeling
  - address issues discovered during data exploration
  - such as select columns in dataset, clean missing data, normalize data
- execute the pipeline
- review tranformed data

#### Regression Models

- supervised ML
- Predict label based on features
- Train model with features and known values
- Use trained model to predict labels for unknown items

| Azure Regression Model| description |
|---|---|
|Bayesian Linear Regression | Linear used for small datasets|
|Boosted Decision Tree Regression | Rapid training and high accuracy, boosted -> each generated tree is based on the last|
|Decision Forest Regression | Rapid training and high accuracy|
|Fast Forest Quantile Regression | For distribution prediction|
|Linear Regression | Rapid training linear model|
|Neural Network Regression | Slow training and high accuracy, allow to create your own custom regression strategy|
|Ordinal Regression | Ranked order datasets|
|Poisson Regression | Event count prediction|

![Bayesian Linear Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Bayesian-Linear-Regression.png)
![Boosted Decision Tree Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Boosted-Decision-Tree-Regression.png)
![Decision Forest Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Decision-Forest-Regression.png)
![Fast Forest Quantile Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Fast-Forest-Quantile-Regression.png)
![Linear Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Linear-Regression.png)
![Neural Network Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Neural-Network-Regression.png)
![Ordinal Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Ordinal-Regression.png)
![Poisson Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Poisson-Regression.png)

- Primary metrics for regression evaluation
  - R Squared/Predicted R Squared,
    - easiest to describe
    - has a python library
    - adjusted R-Squared deals with overfitting
  - Mean Square Error(MSE)/Root Mean Square Error(RMSE)
    - are good for comparing performance between models
  - Mean Absolute Error(MAE)
    - are good for comparing performance between models
  - P-values for independent variables
  - Stepwise regression/best subsets regression

[go to top](#top)

#### Classification models

- supervised ML
- Predict class for items
- Train model with features and known values
- use trained model to prdict labels for unknown items

| Azure Classification Model| description |
|---|---|
|Artificial Neural Network|Naive Bayes|
|Decision tree|Random Forest|
|K-Nearest Neighbor(KNN)|Stochastic Gradient Descent(SGD)随机梯度下降|
|Logistic Regression| Support Vector Machine(SVM)|

![Artificial Neural Network](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Artificial-Neural-Network.png)
![Decision tree](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Decision-tree.png)
![KNN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN.png)
![Logistic Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Logistic-Regression.png)
![Naive Bayes](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Naive-Bayes.png)
![Random Forest](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Random-Forest.png)
![SGD](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/SGD.png)
![SVM](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/SVM.png)

- Possible outcomes of Classification Prediction
  - true positive: prediction and the acural agree on the positive result
  - false positive: prediction is at positive, but the actual data suggests that it should have been negative
  - true negative: prediction and the acural agree on the negative result
  - false negative: prediction is at negative, but the actual data suggests that it should have been positive
- Primary metrics for regression evaluation
  - Accuracy
  - Precision
  - Recall:  the ratio of `true positives/(true negatives + false negatives)`, what the cost of a false negative is.
- Commnon evaluation metrics
  - Logarithmic loss
  - Confusion matrix
  - Area under curce(AUC)
  - F1 score
  - Mean absolute error(MAE)
  - Mean squared error(MSE)
  - Gain and lift charts
  - Kolmogorov-Smirnov charts
  - Log loss
  - Gini coefficient
  - Root Mean squared Error(RMSE)
  - Concordant-discordant ratio

|pipeline of Classification regression| details |pipeline of Classification evaluation| details |
|---|---|---|---|
|sample-pipeline-regression| ![sample-pipeline-regression.png](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-pipeline-regression.png)| sample-evaluation-regression |![sample-evaluation-regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-evaluation-regression.png)|
|sample-pipeline-classification|![sample-pipeline-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-pipeline-classification.png)|sample-evaluation-classification| ![sample-evaluation-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-evaluation-classification.png)|
|sample-inference-classification|![sample-inference-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-inference-classification.png)| | |

[go to top](#top)

#### Clustering models

- unsupervised learning
- useful for identifying partterns
- 聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)。通过这样的划分每个簇可能对应于一些潜在的概念，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名
- types of clustering Algorithm
  - Centroid
  - Density
  - Distribution
  - Hierarchical

|Azure Clustering Model| description |Azure Clustering Model| description |
|---|---|---|---|
|Affinity Propagation|![https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Affinity Propagation](Affinity-Propagation.png) |Agglomerative|![Agglomerative](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Agglomerative.png) |
|BIRCH(Balanced Iterative Reducing and Clustering using Hierachies Model)|![BIRCH](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/BIRCH.png)|DBSCAN(Density-Based Spatial Clustering of Applications with Noise)|![DBSCAN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/DBSCAN.png)|
|K-Means|![K-Means](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/K-Means-cluster.png)|Mean Shift|![Mean Shift](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Mean-Shift.png)|
|OPTICS(Ordering Points to Identify the Clustering Struture)|![OPTICS](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/OPTICS.png)|Caussian Mixture|![Caussian Mixture](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Caussian-Mixture.png)|

- steps for Clustering Mode Training
  - create dataset
  - create pipeline
  - apply transformations
  - run pipeline
  - review tranformed data
- ![pipeline for Clustering Model](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/pipeline-for-Clustering-Model.png) ![evaluation for Clustering Model](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/evaluation-for-Clustering-Model.png)

> references
- [Visualizing K-Means Clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
- [Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)
- [刘建平Pinard-K-Means聚类算法原理](https://www.cnblogs.com/pinard/p/6164214.html)

[go to top](#top)

### Project Jupyter & Notebooks

- Jupyter projecy
  - IPython(Julia, Python, RProducts) project
  - perform ML, Deep learning activities
  - perform data cleansing
  - Statistical modeling
  - Data Visualization
- Jupyter projecy products
  - Jupyter client
  - Jupyter notebook
  - Jupyter Kernels
  - IPykernel package
- Jupter tools
  - JupyterLab
  - nbconvert
  - nbviewer
  - Otconsole
- Jupyter Notebook
  - Web application: client-server App, saved as **ipynb** file
  - document collaboration: Equations, live code, narrative text, visualization
  - Rich text and graphics
  - notebook file(**xxx.ipynb**)
    - Metadata
    - versions
    - cell content
      - cell types
      - Headers
      - text styling
      - lists
      - syntax highlighting

|Kernel| Language |
|---|---|
| IJavascript|Javascript |
|IJulia|Julia|
|IHaskell|Haskell|
|IPHP|PHP|
|IRKernel|R|
|IRuby|Ruby|

|compute type| Description |
|---|---|
|Compute instance|notebooks|
|compute clusters|pipeline|
|inference clusters|inference pipeline|

[go to top](#top)

> references
- [Microsoft ML Learn - Hands On Exercises](https://microsoftlearning.github.io/mslearn-azure-ml/)
- [Azure Machine Learning examples](https://learn.microsoft.com/en-us/samples/azure/azureml-examples/azure-machine-learning-examples/)
