## [Designing and Implementing a Data Science Solution on Azure(DP-100) - Azure Data Scientist Associate](#top)

- [Azure ML workspace](#azure-ml-workspace)
  - [Access to Azure Machine learning workspace](#access-to-azure-machine-learning-workspace)
  - [Azure ML resources](#azure-ml-resources)
  - [Azure ML assets](#azure-ml-assets)
  - [Train Models in the workspace](#train-models-in-the-workspace)
  - [Workspace Tasks](#Workspace-Tasks)
- [Developer tools for workspace interaction](#developer-tools-for-workspace-interaction)
  - [Azure ML Studio](#azure-ml-studio)
  - [Python SDK](#python-sdk)
  - [Work with Azure CLI](#work-with-azure-cli)
  - [Other tools](#Other-tools)
- [Make data available in Azure Machine Learning](#make-data-available-in-azure-machine-learning)
   - [Types of datastores](#types-of-datastores)
  - [URIs](#uris)
  - [Create datastore to an Azure Blob Storage](#create-datastore-to-an-azure-blob-storage)
  - [Create a data asset](#create-a-data-asset)
- [Automated machine learning](#automated-machine-learning)
- [Run a training script as a command job in Azure Machine Learning](#run-a-training-script-as-a-command-job-in-azure-machine-learning)
- [Optimize model training with pipelines in Azure Machine Learning(Machine Learning Operations-MLOps)](#optimize-model-training-with-pipelines-in-azure-machine-learningmachine-learning-operations-mlops)
  - [component](#component)
  - [Pipeline](#pipeline)
- [Azure ML Models](#azure-ml-models)
  - [Steps for training model](#steps-for-training-model)
  - [Regression Models](#regression-models)
  - [Classification models](#classification-models)
  - [Clustering models](#clustering-models)
- [Project Jupyter \& Notebooks](#project-jupyter--notebooks)
- [Azure Data Platform Services](#azure-data-platform-services)
- [Azure Storage Accounts](#azure-storage-accounts)
  - [Azure Blob storage](#azure-blob-storage)
  - [Azure File Shares](#azure-file-shares)
  - [Azure Table storgae](#azure-table-storgae)
  - [Azure Queue storgae](#azure-queue-storgae)
- [Azure Storage Strategy](#azure-storage-strategy)
  - [Data structure type](#data-structure-type)
  - [Azure storage security](#azure-storage-security)
  - [Console application demo by using Visual Studio Code](#console-application-demo-by-using-visual-studio-code)
  - [Azure Storage Services](#azure-storage-services)
- [Azure Data Factory](#azure-data-factory)
  - [Azure Data Factory Elements](#azure-data-factory-elements)
  - [Azure Data Factory Integration Runtime](#azure-data-factory-integration-runtime)
  - [Azure Data Factory Triggers](#azure-data-factory-triggers)
- [Non-relational Data Stores](#non-relational-data-stores)
    - [Azure Cosmos DB](#azure-cosmos-db)
    - [Azure Blob storage](#azure-blob-storage)
    - [Azure Data Lake Storage Gen2](#azure-data-lake-storage-gen2)
    - [Azure Data Masking](#azure-data-masking)
    - [Azure Data Encryption at Rest](#azure-data-encryption-at-rest)
- [Machine Learning Orchestration & Deployment](#machine-learning-orchestration--deployment)
  - [ML pipeline](#ml-pipeline)
  - [Model Features \& Differential Privacy](#model-features--differential-privacy)
  - [ML model Bias](#ml-model-bias)
- [Machine Learning Model Monitoring- Insights](#machine-learning-model-monitoring--insights)
- [Azure Data Storage Monitoring](#azure-data-storage-monitoring)
  - [Azure log analytics Agent](#azure-log-analytics-agent)
  - [Azure Monitor](#azure-monitor)
  - [queries in Azure log analytics](#queries-in-azure-log-analytics)
  - [Montitoring Data Factory pipeline in other way](#montitoring-data-factory-pipeline-in-other-way)
- [Data Solution Optimization](#data-solution-optimization)
  - [Date Partitioning strategies](#date-partitioning-strategies)
  - [Data Solution Optimization](#data-solution-optimization-1)
  - [Azure Blob lifecycle management](#azure-blob-lifecycle-management)
  - [Data Solution Optimization](#data-solution-optimization-2)
- [High Availability \& Disaster Recovery](#high-availability--disaster-recovery)
  - [HADR and SQL Server](#hadr-and-sql-server)
  - [Azure SQL Server backup and restore](#azure-sql-server-backup-and-restore)
  - [Azure availability Groups](#azure-availability-groups)
  - [Azure DB for PostgreSQL availability - Hyperscale(Citus)](#azure-db-for-postgresql-availability---hyperscalecitus)

--------------------------------------------------------------------

### Azure ML workspace

![Azure-ML-workspace1](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace1.png)
![Azure-ML-workspace2](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace2.png)
![Azure-ML-workspace3](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-ML-workspace3.png)

#### Access to Azure Machine learning workspace

- Three general built-in roles
  - Owner: full access to all resources, can grant access to others
  - Contributor: full access to all resources, can't grant access to others
  - Reader: only view the resources
- Azure machine learning's specific built-in roles
  - AzureML Data Scientist
  - AzureML compute Operator
- to fully customize permissions, create a custom role

#### Azure ML resources

- Workspace:  top-level for ML, stores all logs, metrics, outputs, models and snapshots
- Compute resources:
  - compute instance: similar to VM in the cloud, managed by workspace
  - compute cluster: on-demand clusters of CPU or FPU compute nodes in the cloud
  - inference clutster: allows to create or attach Azue Kubernetes Service(AKS) cluster Idela to deploy trained ML models in production scenarios
  - attached compute: allow to attach other Azure compute resources suck as Databricks or Synapse Spark pools
- Datastores: all data is stored in datastores which references to azure data services. When connected to the workspace, two datastores will added to your workspace: **workspaceFilestore** and **workspaceblobstore**

#### Azure ML assets

- Models:
- Environments: stored as an image in Azure Container Registry created with workspace when it's used for the first time
- Data: can use data assets to easily access data every time, without having to provide authentication every time. When create a data assets in the workspace, you'll specify the path to point to the file or folder, name and version.
- Components: make it easier to share code with component in a workspace

#### Train Models in the workspace

- Author and run a pipeline with the designer
- Automated ML: explore algorithms and hyperparameter values
- Jupyter notebook:
  - files:  will stored in the file share of storage account
  - run notebook: use a compute instance
  - can edit and run notebook in Visual Studio Code
- Running a pipeline with the designer
- Run a script as a job
  - command
  - Sweep: perform hyperparameter tuning when executing a single script
  - Pipeline: rung a pipeline consisting of multiple scripts or components

#### Workspace Tasks

- Run experiment to train model
- User automated ML to train model
- Register a model
- Deploy a model
- Create and run workflows
- View ML artifacts
- Monitor models
  
[go to top](#top)

 ### Developer tools for workspace interaction

 #### Azure ML Studio

 - launch from overview page
 - https://ml.azure.com/

#### Python SDK

```shell
# 1 Install the Python SDK
pip install azure-ai-ml
# 2 Connect to the workspace
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
# 3 sample- connect to the workspace when you create a new job to train a model
from azure.ai.ml import command
# 3.1 configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    experiment_name="train-model"
)
# 3.2 connect to workspace and submit job
returned_job = ml_client.create_or_update(job)
```

> reference
- [The reference documentation on the MLClient class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient?view=azure-python)
- [The reference documentation also includes a list of the classes for all entities](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities?view=azure-python)
- [AmlCompute Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.amlcompute?view=azure-python)

#### Work with Azure CLI

```shell
# install Azure Machine Learning extension
az extension add -n ml -y
# Work with the Azure CLI,  Each command is prefixed with az ml
az ml compute create --name aml-cluster --size STANDARD_DS3_v2 --min-instances 0 --max-instances 5 --type AmlCompute --resource-group my-resource-group --workspace-name my-workspace
# YAML schemas - compute.yml
$schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json
name: aml-cluster
type: amlcompute
size: STANDARD_DS3_v2
min_instances: 0
max_instances: 5
# create compute by using yml file
az ml compute create --file compute.yml --resource-group my-resource-group --workspace-name my-workspace
```

#### Other tools

- Azure ML SDK for R
- Azure ML VS code extension

> reference
- [az ml commands](https://learn.microsoft.com/en-us/cli/azure/ml?view=azure-cli-latest)
- [az ml commands(v1)](https://learn.microsoft.com/en-us/cli/azure/ml(v1)?view=azure-cli-latest)
- [YAML schemas](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-overview?view=azureml-api-2)

[go to top](#top)

### Make data available in Azure Machine Learning

#### Types of datastores

- Authentication methods
  - Credential-based: Use a service principal, shared access signature (SAS) token or account key to authenticate access to your storage accoun
  - Identity-based:  Use your Azure Active Directory identity or managed identity
- Types of datastores
  - Azure Blob storage
  - Azure File share
  - Azure Data Lake(Gen 1)
  - Azure Data Lake(Gen 2)
- Every workspace has **four built-in datastores** (two Azure Storage blob containers, and two Azure Storage file shares)

#### URIs

- http(s): data stores publicly or privately in an Azure Blob storage or publicly available http(s) location
- wasb(s): data stores in Azure Blob Storage
  - `wasbs://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>`
- abfs(s): data stores in an Azure Data Lake Storage Gen 2
  - `abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>`
- azureml(datastore): data stores in an Azure ML datastore, A datastore is a reference to an existing storage account on Azure
  - `azureml://datastores/<datastore_name>/paths/<folder>/<file>`
- Local: `./<path>`

#### Create datastore to an Azure Blob Storage

- [Create datastores](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2&tabs=sdk-identity-based-access%2Csdk-adls-identity-access%2Csdk-azfiles-accountkey%2Csdk-adlsgen1-identity-access)


#### Create a data asset

- main types of data asset
  - file:
  - Table: points to a folder or file, and includes a schema to read as tabular data
  - When you create a data asset and point to a file or folder stored on your local device, a copy of the file or folder will be uploaded to the default datastore workspaceblobstore. You can find the file or folder in the LocalUpload folder. By uploading a copy, you'll still be able to access the data from the Azure Machine Learning workspace, even when the local device on which the data is stored is unavailable

```python
# 1 Create a URI file data asset
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
my_path = '<supported-path>'
my_data = Data(
    path=my_path,
    type=AssetTypes.URI_FILE,
    description="<description>",
    name="<name>",
    version="<version>"
)
ml_client.data.create_or_update(my_data)
# read the data from URI file data asset(points to a CSV file)
import argparse
import pandas as pd
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()
df = pd.read_csv(args.input_data)
print(df.head(10))
# 2 Create a URI folder data asset
import argparse
import glob
import pandas as pd
parser = argparse.ArgumentParser()
parser.add_argument("--input_data", type=str)
args = parser.parse_args()
data_path = args.input_data
all_files = glob.glob(data_path + "/*.csv")
df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)
# 3 Create a MLTable data asset
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
my_path = '<path-including-mltable-file>'
my_data = Data(
    path=my_path,
    type=AssetTypes.MLTABLE,
    description="<description>",
    name="<name>",
    version='<version>'
)
ml_client.data.create_or_update(my_data)
```

[go to top](#top)

### Automated machine learning

1. prepare data- create data asset
2. scaling and normalization to numeric data automatically, You can choose to have AutoML apply preprocessing transformations, such as:
   1. Missing value imputation to eliminate nulls in the training dataset
   2. Categorical encoding to convert categorical features to numeric indicators
   3. Dropping high-cardinality features, such as record IDs
   4. Feature engineering (for example, deriving individual date parts from DateTime features)
3. run Automated Machine Learning- configure and submit the job with the Python SDK
   1. Restrict algorithm selection(optional)
   2. Configure an AutoML experiment(Python SDK v2)
   3. Set the limits(optional) by using `set_limits()`
      - `timeout_minutes`: Number of minutes after which the complete AutoML experiment is terminated
      - `trial_timeout_minutes`: Maximum number of minutes one trial can take
      - `max_trials`: Maximum number of trials, or models that will be trained
      - `enable_early_termination`: Whether to end the experiment if the score isn't improving in the short term
   4. Set the training properties(optional)
   5. Submit an AutoML experiment


```python
# 1 create MLTable file stored in a folder
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import Input
my_training_data_input = Input(type=AssetTypes.MLTABLE, path="azureml:input-data-automl:1")
# 3.1 configure the classification job
from azure.ai.ml import automl
classification_job = automl.classification(
    compute="aml-cluster",
    experiment_name="auto-ml-class-dev",
    training_data=my_training_data_input,
    target_column_name="Diabetic",
    primary_metric="accuracy",
    n_cross_validations=5,
    enable_model_explainability=True
)
# 3.3 set limits to an AutoML experiment or job
classification_job.set_limits(
    timeout_minutes=60,
    trial_timeout_minutes=20,
    max_trials=5,
    enable_early_termination=True,
)
# 3.5 submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    classification_job
)
# 3.5 monitor AutoML job runs in the Azure Machine Learning studio
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

> reference
- [how to create a MLTable data asset in Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-mltable?view=azureml-api-2&tabs=cli)
- [overview of supported algorithms](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train?view=azureml-api-2#supported-algorithms?azure-portal=true)


4. Evaluate and compare models after automated ML
5. Retrieve the best run and its models
   - [Evaluate automated machine learning experiment results](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2)

[go to top](#top)

### Run a training script as a command job in Azure Machine Learning

- notebook are ideal for exploration and development, Scripts are ideal for testing and automate in production env
- to create a production-ready scipt, need to
  - remove nonessential code
  - refactor code into functions
  - test script(in the terminal)
- Run a script as a command job(To run a script as a command job, you'll need to configure and submit the job). Use the `command` function and need to specify values for the following parameters:
  - `code`: The folder that includes the script to run
  - `command`: Specifies which file to run
  - `environment`: The necessary packages to be installed on the compute before running the command
  - `compute`: the compute to use to run the command
  - `display_name`: The name of the individual job
  - `experiment_name`: The name of the experiment the job belongs to
  - [the command function and all possible parameters](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml?view=azure-python)
- `git clone https://github.com/MicrosoftLearning/mslearn-dp100`

```python
from azure.ai.ml import command
# configure job
job = command(
    code="./src",
    command="python train.py --training_data diabletes.csv",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-model",
    experiment_name="train-classification-model"
    )
# submit job
returned_job = ml_client.create_or_update(job)
```

[go to top](#top)

### Optimize model training with pipelines in Azure Machine Learning(Machine Learning Operations-MLOps)

- **Pipelines** is a workflow of machine learning tasks in which each task is defined as a component, it contains steps related to the training of a machine learning model
- **Components** allow you to create reusable scripts that can easily be shared across users within the same Azure Machine Learning workspace, can use components to build an Azure ML pipeline

#### component

- A component consists of three parts:
  - Metadata: Includes the component's name, version, etc
  - Interface: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts)
  - Command, code and environment: Specifies how to run the code
- To create a component, need two files:
  - A script that contains the workflow
  - A YAML file to define the metadata, interface, and command, code, and environment of the component. Use the `command_component()` function as a decorator to create the YAML file
- [Create and run machine learning pipelines using components with the Azure Machine Learning SDK v2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2)

```python
# 1 prepares the data by removing missing values and normalizing the data
# import libraries
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler
# setup arg parser
parser = argparse.ArgumentParser()
# add arguments
parser.add_argument("--input_data", dest='input_data', type=str)
parser.add_argument("--output_data", dest='output_data', type=str)
# parse args
args = parser.parse_args()
# read the data
df = pd.read_csv(args.input_data)
# remove missing values
df = df.dropna()
# normalize the data
scaler = MinMaxScaler()
num_cols = ['feature1','feature2','feature3','feature4']
df[num_cols] = scaler.fit_transform(df[num_cols])
# save the data as a csv
output_df = df.to_csv(
    (Path(args.output_data) / "prepped-data.csv"),
    index = False
)
# 2 create a component for the prep.py script with prep.yml
# 2.1 prep.yml
$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: prep_data
display_name: Prepare training data
version: 1
type: command
inputs:
  input_data:
    type: uri_file
outputs:
  output_data:
    type: uri_file
code: ./src
environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest
command: >-
  python prep.py
  --input_data ${{inputs.input_data}}
  --output_data ${{outputs.output_data}}
# 2.2 load the component
from azure.ai.ml import load_component
parent_dir = ""
loaded_component_prep = load_component(source=parent_dir + "./prep.yml")
# 2.3 register a component
prep = ml_client.components.create_or_update(prepare_data_component)
```

#### Pipeline

- pipeline is a workflow of machine learning tasks in which each task is defined as a component
- Components can be arranged sequentially or in parallel
- Each component can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal
- A pipeline can be executed as a process by running the pipeline as a pipeline job
- Each component is executed as a child job as part of the overall pipeline job
- create a pipeline
  - using YAML file
  - using `@pipeline()`
  - ![pipeline-built](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/pipeline-built.png)
- run a pipeline job
  - configure a pipeline job: defined as a YAML file
  - run a pipeline job
  - schedule a pipline job: by using `JobSchedule` class 

[go to top](#top)

### Azure ML Models

#### Steps for training model

- create a pipeline
- add and explore a dataset
  - train regression model
  - make a prediction based on characteristics
  - azure ML has sample dataset
- apply data transformations
  - prepare data for modeling
  - address issues discovered during data exploration
  - such as select columns in dataset, clean missing data, normalize data
- execute the pipeline
- review tranformed data

#### Regression Models

- supervised ML
- Predict label based on features
- Train model with features and known values
- Use trained model to predict labels for unknown items

| Azure Regression Model| description |
|---|---|
|Bayesian Linear Regression | Linear used for small datasets|
|Boosted Decision Tree Regression | Rapid training and high accuracy, boosted -> each generated tree is based on the last|
|Decision Forest Regression | Rapid training and high accuracy|
|Fast Forest Quantile Regression | For distribution prediction|
|Linear Regression | Rapid training linear model|
|Neural Network Regression | Slow training and high accuracy, allow to create your own custom regression strategy|
|Ordinal Regression | Ranked order datasets|
|Poisson Regression | Event count prediction|

![Bayesian Linear Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Bayesian-Linear-Regression.png)
![Boosted Decision Tree Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Boosted-Decision-Tree-Regression.png)
![Decision Forest Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Decision-Forest-Regression.png)
![Fast Forest Quantile Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Fast-Forest-Quantile-Regression.png)
![Linear Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Linear-Regression.png)
![Neural Network Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Neural-Network-Regression.png)
![Ordinal Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Ordinal-Regression.png)
![Poisson Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Poisson-Regression.png)

- Primary metrics for regression evaluation
  - R Squared/Predicted R Squared,
    - easiest to describe
    - has a python library
    - adjusted R-Squared deals with overfitting
  - Mean Square Error(MSE)/Root Mean Square Error(RMSE)
    - are good for comparing performance between models
  - Mean Absolute Error(MAE)
    - are good for comparing performance between models
  - P-values for independent variables
  - Stepwise regression/best subsets regression

```
|             Metrics         |             description                     
|-----------------------------|-------------------------------------------------------------------------------------------------
|Negative log likelihood      | measures the loss function, a lower score is better
|                             | note: only for Bayesian Linear Regression and Decision Forest Regression; for other algorithms, the value is Infinity which means for nothing
|Mean absolute error(MAE)     | measures how close the predictions are to the actual outcomes; a lower score is better
|Root mean squared error(RMSE)| errors number in the model, disregards the difference between over-prediction and under-prediction
|Relative absolute error(RAE) | relative absolute difference between expected and actual values
|Relative squared error(RSE)  | normalizes the total squared error of the predicted values by dividing by the total squared error of the actual values.
|Mean Zero One Error(MZOE)    | indicates whether the prediction was correct or not.  ZeroOneLoss(x,y) = 1 when x!=y; otherwise 0.
|Coefficient of determination | R^2
```

[go to top](#top)

#### Classification models

- supervised ML
- Predict class for items
- Train model with features and known values
- use trained model to prdict labels for unknown items

| Azure Classification Model| description |
|---|---|
|Artificial Neural Network|Naive Bayes|
|Decision tree|Random Forest|
|K-Nearest Neighbor(KNN)|Stochastic Gradient Descent(SGD)随机梯度下降|
|Logistic Regression| Support Vector Machine(SVM)|

![Artificial Neural Network](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Artificial-Neural-Network.png)
![Decision tree](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Decision-tree.png)
![KNN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN.png)
![Logistic Regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Logistic-Regression.png)
![Naive Bayes](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Naive-Bayes.png)
![Random Forest](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Random-Forest.png)
![SGD](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/SGD.png)
![SVM](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/SVM.png)

- Possible outcomes of Classification Prediction
  - true positive: prediction and the acural agree on the positive result
  - false positive: prediction is at positive, but the actual data suggests that it should have been negative
  - true negative: prediction and the acural agree on the negative result
  - false negative: prediction is at negative, but the actual data suggests that it should have been positive
- Primary metrics for regression evaluation
  - Accuracy
  - Precision
  - Recall:  the ratio of `true positives/(true negatives + false negatives)`, what the cost of a false negative is
 
**Metrics in Evaluate Model**

```
|     Metrics     |             description                     
|-----------------|-------------------------------------------------------------------------------------------------
| Accuracy        | proportion of true results to total cases
| Precision       | proportion of true results over all positive results
| Recall          | fraction of all correct results returned by the model
| F-score         | weighted average of precision and recall between 0 and 1, where the ideal value is 1 
| AUC             | area under the curve plotted with true positives on the y axis and false positives on the x axis,
|                 | it provides a single number that lets you compare models of different types.
|Average log loss | difference between two probability distributions – the true one, and the one in the model, used to express the penalty for wrong results
|Training log loss| comparing the probabilities it outputs to the known values (ground truth) in the labels
```

|pipeline of Classification regression| details |pipeline of Classification evaluation| details |
|---|---|---|---|
|sample-pipeline-regression| ![sample-pipeline-regression.png](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-pipeline-regression.png)| sample-evaluation-regression |![sample-evaluation-regression](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-evaluation-regression.png)|
|sample-pipeline-classification|![sample-pipeline-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-pipeline-classification.png)|sample-evaluation-classification| ![sample-evaluation-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-evaluation-classification.png)|
|sample-inference-classification|![sample-inference-classification](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/sample-inference-classification.png)| | |

[go to top](#top)

#### Clustering models

- unsupervised learning
- useful for identifying partterns
- 聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)。通过这样的划分每个簇可能对应于一些潜在的概念，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名
- types of clustering Algorithm
  - Centroid
  - Density
  - Distribution
  - Hierarchical

|Azure Clustering Model| description |Azure Clustering Model| description |
|---|---|---|---|
|Affinity Propagation|![https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Affinity Propagation](Affinity-Propagation.png) |Agglomerative|![Agglomerative](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Agglomerative.png) |
|BIRCH(Balanced Iterative Reducing and Clustering using Hierachies Model)|![BIRCH](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/BIRCH.png)|DBSCAN(Density-Based Spatial Clustering of Applications with Noise)|![DBSCAN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/DBSCAN.png)|
|K-Means|![K-Means](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/K-Means-cluster.png)|Mean Shift|![Mean Shift](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Mean-Shift.png)|
|OPTICS(Ordering Points to Identify the Clustering Struture)|![OPTICS](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/OPTICS.png)|Caussian Mixture|![Caussian Mixture](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Caussian-Mixture.png)|

- steps for Clustering Mode Training
  - create dataset
  - create pipeline
  - apply transformations
  - run pipeline
  - review tranformed data
- ![pipeline for Clustering Model](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/pipeline-for-Clustering-Model.png) ![evaluation for Clustering Model](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/evaluation-for-Clustering-Model.png)

> references
- [Visualizing K-Means Clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
- [Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)
- [刘建平Pinard-K-Means聚类算法原理](https://www.cnblogs.com/pinard/p/6164214.html)

[go to top](#top)

### Project Jupyter & Notebooks

- Jupyter projecy
  - IPython(Julia, Python, RProducts) project
  - perform ML, Deep learning activities
  - perform data cleansing
  - Statistical modeling
  - Data Visualization
- Jupyter projecy products
  - Jupyter client
  - Jupyter notebook
  - Jupyter Kernels
  - IPykernel package
- Jupter tools
  - JupyterLab
  - nbconvert
  - nbviewer
  - Otconsole
- Jupyter Notebook
  - Web application: client-server App, saved as **ipynb** file
  - document collaboration: Equations, live code, narrative text, visualization
  - Rich text and graphics
  - notebook file(**xxx.ipynb**)
    - Metadata
    - versions
    - cell content
      - cell types
      - Headers
      - text styling
      - lists
      - syntax highlighting

|Kernel| Language |
|---|---|
| IJavascript|Javascript |
|IJulia|Julia|
|IHaskell|Haskell|
|IPHP|PHP|
|IRKernel|R|
|IRuby|Ruby|

|compute type| Description |
|---|---|
|Compute instance|notebooks|
|compute clusters|pipeline, experiments, endpoint|
|inference clusters|inference pipeline|

- PyTorch: 既可以看作加入了GPU支持的numpy，同时也可以看成一个拥有自动求导功能的强大的深度神经网络
  - 已经被Facebook, Twitter、CMU和Salesforce等机构采用
- TensorFlow: 是谷歌基于DistBelief进行研发的第二代人工智能学习系统
  - Tensor(张量)意味着N维数组，Flow(流)意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端的计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。
 TensorFlow可被用于语音识别和图像识别等多项机器学习和深度学习领域，对2011年开发的深度学习基础架构DistBelief进行了各方面的改进，它可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行。
  - 支持CNN(卷积神经网络)、RNN(循环神经网络)和LSTM(长短期记忆网络)算法，是目前在 Image，NLP最流行的深度神经网络模型

[go to top](#top)

### Azure Data Platform Services

```
                                            Azure Data Platform Services
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Managed relational DB                   |  requirement for on-demanded scaling, no downtime, intelligent optimization
  Azure SQL DB    | Structured and unstructured             |  focus on security and availability
                  | Online Transaction Processing(OLTP)     |  Reduced costs associated with on-site infrastructure
                  | Strong security and availability        |  protected by SLA(Azure service Level Agreement)
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Globally distributed                    |  requirement for NoSQL
                  | Multimodel                              |  focus on global scale, regional and automatic failover
Azure Cosmos DB   | API models                              |  need for low latency
                  |  - SQL API, MongoDB API, Gremin API,    |  <1 second response time, 10ms read/write guarantee
                  |  - Table API, Cassandra API             |  strong consistency levels
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Azure base storage type                 |  need to store but not query data
                  |   -Blob, files, queue, table            |  need to work with unstructured data and images
Azure Data Storage| scalable object store                   |  require a cost-effective solutions
                  | Messaging store                         |  scalable and highly available, no downtime
                  | NoSQL store                             |  Azure-managed hardware maintenance, REST API and SDKs
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Hadoop-compatiable                      |  big data analytics
                  |   -Blob, files, queue, table            |  Hierarchal filesystem
Azure Data Lake   | scalable object store                   |  Azure blob storage
Storage Gen2      | Messaging store                         |  scalable and highly available, no downtime
                  | NoSQL store                             |  Azure-managed hardware maintenance, REST API and SDKs
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Data warehousing                        |  unified management, monitoring, security
Azure Synapse     | big data analytics                      |  Massively parallel processing(MPP)
Analytics         | Unlimited scaling                       |  Synapse SQL, Apache Spark for Azure Synapse
                  | Strong security and availability        |  Data Lake functionality,
                  |                                         |  Synapse Studio
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | Internet of Things(IoT)                 |  Real-time response, continuous data monitoring
Azure Stream      | web logs                                |  streaming or batch data
Analytics         | Ongoing monitoring                      |  Data Lake functionality,
                  | Point of Sale(POS)                      |  Synapse Studio
                  | Extremely cost effective                |
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | data warehousing tool                   |  Azure Databricks Integration
                  | Apache Spark capable                    |    - Azure Synapse analytics, Azure Data Lake storage
Azure Databricks  | managed Spark clusters                  |    - Azure CosmosDB, Azure Blob storage, Power BI
                  | managed SQL endpoints,Alerts,Dashboards |
                  | Azure databricks notebooks              |
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | cloud-based ETL and integration platform|  Use Azure HDInsight, Spark, Hadoop, Azure ML
Azure Data Factory| Control data movement between stores    |  Public to Azure SQL data warehouse
                  | managed Spark clusters                  |  Organize raw data
------------------|-----------------------------------------|----------------------------------------------------------------------------
                  | wrapped Hadoop, HBase, Storm, Kafka     |  Cloud native, Scalability and cost, Monitoring, Productivity, Extensibility,
Azure HDInsight   | Control data movement between stores    |  Security and compliance, Global availability
                  | managed Spark clusters                  |
------------------|-----------------------------------------|----------------------------------------------------------------------------
```
- Azure Data Lake Gen2 Features
  - Unlimited scaling
  - Hadoop-compatible
  - Access control list(ACL) support
  - Zone-redundant storage
  - Geo-redundant storage
  - POSIX compliant
  - Optimized Azure Blob File System(ABFS) driver
- Azure Databricks
  - Azure Databricks Workspace
    - Spark SQL and dataFrames
    - Streaming
    - MLib
    - GraphX
    - Spark Core API
  - Managing Spark Clusters
    - Quickly create clusters
    - Autoscaling up and down
    - Share clusters across teams
    - Program clusters using REST APIs
    - Secure data integration
    - No need to centralize unified data
    - Automatic Spark updating
- Azure Data Factory
  - How Azure Data Factory works
    - Connect and collect
    - Transform and enhance
    - CI/CD and publish
    - Monitor
  - Azure Data Factory components
    - pipelines, Activities, Datasets, linked services, Data flows, Integration runtimes

#### Security of Data Platform Services

|DB| Security |DB| Security |
|---|---|---|---|
| |Advanced Threat Protection(ATP)| | Automatic data encryption|
| |Strong encryption| |IP firewall|
|Azure SQL DB|SQL DB auditing|Azure Cosmos DB| Virtual networks|
| |Multi-factor authentication(MFA)| | Token-based user authentication|
| |Azure Active Directory(ADD) authentication| |Role-based security with ADD|
| |Compliance certification| |Compliance certification|
|------------------|------------------------------------|------------------|------------------------------------|
| |Data encryption | |Automatic encryption at rest|
| |Precise access control | |Active Directory(AD) security groups |
|Azure data storage |Keys or shared access signatures|Azure Data Lake Gen2 |Role-based access control(RBAC)|
| |Azure Resource Manager| |Built-in security groups |
| |Role-based access control(RBAC)| |firewall |
|------------------|-----------------------------------|------------------|------------------------------------|
| |Azure Active Directory(ADD) | |Transport layer between device and IoT hub |
| |Multi-factor authentication | |Shared key to secure transfer|
|Azure Synapse Analytics | Column-based security |Azure Stream analytics |Streaming data ignored |
| |Row-based security | | Data security provided by storage device |
| |SQL Server authentication |  |   |
|------------------|-----------------------------------|------------------|------------------------------------|
| |Azure active Directory | |Transport layer between device and IoT hub |
| |Role-based access | |Shared key to secure transfer|
|Azure databricks |enterprise-grade service level agreements(SLAs)|Streaming data ignored |
| |Column-based security | | Data security provided by storage device |
| |Row-based security| |  |
|------------------|-----------------------------------|------------------|------------------------------------|
| |Encryption | |  |
|Azure HDInsight |Secure Shell(SSH) | |  |
| |Shared access signatures| | |
| |Azure Active Directory(AAD) | |  |

#### Working with Data in Data Platform Services

|Working with| description |Working with| description |
|---|---|---|---|
| |Software development Kits(SDKs) | | Azure Data Factory|
|Azure SQL DB |.Net, Python, Java, NodeJS|Azure Cosmos DB |Azure Cosmos DB API|
| |Transact-SQL(T-SQL) | |JSON|
| | Azure Data Factory| | Direct editing|
|------------------|------------------------------------|------------------|------------------------------------|
| |AzCopy | | Apache sqoop|
| |Azure Data Factory |  |AzCopy|
|Azure Data storage |Powershell |Azure data lake storage Gen2 |Azure Data Factory|
| |Storage explorer | | Azure storage explorer|
| |Visual studio | | Powershell|
| | | |Visual studio|
|------------------|------------------------------------|------------------|------------------------------------|
| |Extract, load, transform(ELT) | |Azure Event Hubs |
|Azure Synapse Analytics |PolyBase |   |Azure IoT hub|
| |Azure Data Factory | |Azure Blob storage |
|------------------|------------------------------------|Azure Stream analytics |Inut and output pipelines |
|Azure data factory |Hive for ETL operations | |Route job output |
| |Azure data factory for Hive queries | |Batch analytics |
| | | |Consume with other service |
| | | |Send to Power BI |
|------------------|------------------------------------|------------------|------------------------------------|
| |Java and Python | | |
|Azure HDInsight |Mapper, Reducer | | |
| |Spark, Anaconda libraries | | |
| |GraphX, Storm | |  |

[go to top](#top)

### Azure Storage Accounts

**Storage acount setting**

| setting|Description|
|---|---|
| Subscription  |Billing account  |
|location| Data center|
|Performance|Data services and hard disks|
|Replication|Backup|
|Access tier| Determines speed of blob access|
|Secure transfer required| HTTP or HTTPS|
|Virtual networks|Limit inbound access to virtual networks|

- Account type
  - storageV2
  - storage
  - Blob storage

#### Azure Blob storage

- cloud object storage
- Massive unstructured data

```
-------------|---------- uses -----------------------------|-------- Accessing Object -----------------------
             | serving files to browsers                   |  HTTP/HTTPS
             | storage for distributed access              |  Azure storage REST API
             | streaming audio and video                   |  Azure PowerShell
Blob storage | streaming audio and video                   |  Azure CLI
             | Log file writing                            |  Azure storage client library
             | Backup, disaster recoverty, archiving       |
             | analysis by on-site or Azure-hosted service |
-------------|---------------------------------------------|-------------------------------------------------
```

- Azure Blob storage and Azure Data Lake storage Gen2
  - Tierd storage
  - High availability
  - Consistent
  - Disaster recovery
- Azure Blob resources
  - Storage accounts
    - unique namespace
    - objects include account name
    - object base address
  - Containers
    - all blobs must be stored in a blob container
    - used to organize blobs
    - unlimited containers for storage accounts
    - unlimited blobs for containers
  - Blobs types
    - Block blobs
    - append blobs
    - page blobs
  - Moving data to blob storage
    - AzCopy
    - Azure Data factory
    - Azure Data Box
    - Azure import/export service
    - Azure storage Date movement library
    - Blobfuse

#### Azure File Shares

- features
  - Fully managed shares
  - Mounted concurrently
  - Serve Message Block(SMB)
  - Networkd File System(NFS)
- uses
  - replacing or complimenting on-site hardware
  - lifting and shifting apps
  - containerizing
  - simplifying development
- Options
  - Premium, Transaction-optimized, Hot, Cool
- Azure fiel shares and cloud development
  - development, testing, debugging
  - shared applications
  - diagnostics
- soft delete: need to take snapshots periodically

#### Azure Table storgae

- feature
  - Structured NoSQL data
  - key/attribute store
  - Schema-free design
- Table storage
  - store and query massive amounts of data
  - tables scale with demand
  - flexible dataset storage
  - unlimited number of enitites in a table
  - unlimited number of tables in storage accounts
- Azure Table storage components
  - URL format
  - Account
  - Table
  - Entity
  - Properties
- Common User cases for Azure table storage
  - serving web application
  - fast queries with clustered index
  - Datasets that can be denormalized
  - Using OData and LINQ queries wiht WCF data service .NET libraries

#### Azure Queue storgae

- features
  - large amount of **messages**
  - messages can be accessed from anywhere
  - HTTP or HTTPS
  - Messages up to 64KB
- Azure Queue storage components
  - URL format
  - Storage account
  - Queue
  - Message
- Advantages
  - Large workloads
  - Client libraries
  - REST API access
  - decoupling components
  - Resilient application development
  - Burst scaling

[go to top](#top)

### Azure Storage Strategy

- determining Data and performance requirement
  - simple lookups
  - database queries
  - creation, updating, deletion activities
  - complex data analysis
  - speed of data operations
- Transactions
  - group of operations
  - group execution
  - ACID(Atomicity, Consistency, Isolation, Durability)
  - OLTP or OLAP
- Azure Core Storage Services
  - Azure blobs
  - Azure files
  - Azure Queues
  - Azure Tables
  - Azure Disks
- Encryption for core storage services
  - At rest
  - intraction(Client-side)

|Data type|Structure |Service|
|---|---|---|
|Product catalogs |Semi-structured|Azure Comsmos DB|
|Video and photographs|Unstructured|Azure Blob storage|
|Business data|Strutured| Azure SQL Database|


####  Data structure type

```
--------------------|--------------------------------------|------------------------------------
                    | relational model                     | slow when changes made
                    | Table structure                      | bulk update for new columns
 Structured data    | Column width                         | use query language
                    | Defined at time of design            |
                    | Designed before data is populated    |
--------------------|--------------------------------------|------------------------------------
                    | Binary, audio, image files           | key-value store
                    | No-relational(NoSQL)                 | document database
 Unstructured data  | Raw data format                      | graph database
                    | not defined at time of design        | column database
                    | structure defined when data read     |
--------------------|--------------------------------------|------------------------------------
                    | Less organized than structured data  |
Semi-structured data| organization through tags            |
                    | handled by No-relational(NoSQL) DB   |
--------------------|--------------------------------------|------------------------------------
```

#### Azure storage security

- Advantages
  - Encrpytion at rest
  - Encrpytion in transit
  - User access control
  - Browser cross-domain access
  - Storage access auditing
- Blob and Queue storage security
  - Active Directory(AD) authorization
  - Access control for blobs and Queue storags
- Storage account keys
  - primary and secondary key
  - Blobs, files, queues, tables
  - Easy to implement
  - HTTP authorization header
  - Protecting Shared Keys
- Shared Access Signatures(SAS)
  - Untrusted clients
  - control storage object access
  - Security token attached to URI
  - Types of SAS
    - Service-level SAS
    - Account-level SAS
- Controlling storage acount network access
  - manage default rules
  - ensure necessary services have access
- Azure Storage Advanced Threat Protection(ATP)
  - Unexpected behavior detection
  - Security alerts
  - Azure Defender for Storage(Analyse tool)
  - Hierarchal namespaces

#### Console application demo by using Visual Studio Code

```shell
# open new terminal in Visual Studio Code
donet new console   #create new console application
mkdir data          #create new directory to story data
donet add package Azure.Storage.Blobs
#edit program.cs
using System;
using System.Threading.Tasks;
using Azure.Storage.Blobs;

namespace sbdemo10_05
{
  class program
  {
    static async Tak Main(string[] args)
    {
      var connectionString = "xxxxxxxxxxxxxxxxxxxxxxxx";
      var container = new BlobContainerClient(connectionString, "demo");
      await container.CreateIfNotExistsAsync();
      # create a new file demo.txt in data directory
      var fileName = "demo.txt";
      var blob = container.GetBlobClient(fileName);
      await blob.UploadAsync(Path.combine("data", fileName), true);
      await blob.DownloadAsync(Path.combine("download", fileName));
    }
  }
}
# back to terminal in Visual Studio Code
dotnet run
```

#### Azure Storage Services

- Features
  - Managed
  - Scalable
  - Secure
  - Durable
- Azure data Types
  - Blobs
  - Files
  - Queues
  - Tables storage
- Azure storage Limits
  - 200 storage accounts
  - 500 TB per account
- Azure storage accounts
  - General-purpose v2(GPv2)
  - General-purpose v1(GPv1)
  - Blob storage accounts
- Azure Disk storage types
  - Managed
  - Unmanaged
- Azure Disk Types
  - Standard
  - Premium

[go to top](#top)

### Azure Data Factory

- SQL server
- Data management gateway
- Azure Storage
- Azure SQL database
- ETL(Extract, Transform, Load), ELT(Extract, Load, Transform)
- Data-driven workflows, pipeline, dataset, activity
- Mapping data flow, control flow, trigger
- Linked service, connect
- Transform and enhance
- Continuous Integration(CI) and Continuous Delivery(CD)
- Monitoring
-
#### Azure Data Factory Elements

- Activity: step in the workflow
- Linked service:
- Integartion Runtime(IR): provides the bridge between the activity and Linked service

#### Azure Data Factory Integration Runtime

- IR is the underlying compute infrastructure that data factory uses to execute workflows across different networks
  - data flow
  - data movement
  - Activity dispatch: executing tranformation activities on various other services like databricks, HDInsight, ML
  - SSIS package execution
- how to using
  - Azure Data Factory UX
  - Management hub
- Types of IR
  - Azure
  - Self-hosted: cannot perform data flow
  - Azure SQL Server Integation Services(SSIS)
- Azure IR activities
  - Execute data flows using Azure
  - Execute copy activity between data stores
  - Apply transform activities
- Azure IR Networking
  - connect to data stores and computing service
  - connect with private link using Managed Virtual Network
- Azure IR Resources and scaling
  - managed and serverless
  - secure, high performance data movement
  - Lightweight routing with activity dispatch, it is no need to scale compute
- Azure IR Linked services and datasets
  - ![Azure IR Linked services](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Azure-IR-Linked-services.png)

#### Azure Data Factory Triggers

- types
  - manual: powershell, .NET SDK, REST API, Python SDK
  - scheduled
  - event-driven:
- pipelines and triggers
  - many-to-many relationship
  - multiple triggers can execute single pipeline
  - single trigger can execute multiple pipeline
- JSON properties for scheduling triggers
  - startTime, endTime, timeZone
  - recurrence, frequency, interval, schedule

[back to top](#top)

### Non-relational Data Stores

 - Types
   - Columnar
   - Document
   - External index
   - Graph
   - Key/value
   - Object
   - Time series
 - Unstructured Data formats
   - CSV(Comma-separated values)
   - JSON(Javascript Object Notation)

#### Azure Cosmos DB

  - NoSQL DB
  - Globally distributed,
    - regional failover, automatic failover, Multmaster replication
  - Multimodel
  - 99.999 percent uptime
  - Low latency
  - response time under 10ms
 - working with Azure Cosmos DB
   - Data ingestion
   - Data security
   - Queries
 - Azure Cosmos DB API Models
   - SQL API
   - MongoDB API
   - Gremlin API: graph API
   - Table API
   - Cassandra API
- Azure Cosmos DB schema and JSON
 - Schema-free
 - Automatic indexing
 - come in and out respented as JSON documents(node) and documents converted to trees
- Azure Cosmos DB Index types
  - range: great in tree structure, used for scalar values like strings and numbers
  - spatial: allow to search for data based on mathematical relationship to other data, such as distance and intersect
  - composite

#### Azure Blob storage

- Features
  - serving images and documents to a browser
  - audio and video streaming
  - distributed access
  - writing to log files
  - backup, restore, archiving
  - data analysis
- Access to blob storage
  - HTTP(S)
  - Azure Storage REST API
  - Azure Powershell
  - Azure CLI
  - Azure storage client library: .NET, JAVA, Nodejs, Python, Go, PHP, Ruby
- Azure Blob storage resource
  - Storage account
  - container
  - Blob
- Azure Blob storage security
  - Data protection
  - Identity and access management(IAM)
  - Networking, such as firewall
  - Monitoring and logging

#### Azure Data Lake Storage Gen2

- Big data analytis -> Massive amounts of data, massive throughput
- Hierarchal namespaces
- Azure blob storage
- work with Azure Data Lake Storage Gen2
  - Ingesting
  - processing
  - downloading
  - visualizing
- Common data types
  - ad hoc
  - azure HDInsight clusters
  - On-sites or IaaS Hadoop clusters
  - Relational
  - streamed
  - Very large datasets
  - Web server logs
- Processing Data
  - Azure HDInsight
  - Azure Databricks
- Visualing Data
  - Power BI connector
  - Generate visual representations
- Downloading Data
  - Azure data factory
  - Azure storage explorer
  - Apache DistCp
  - AzCopy

#### Azure Data Masking

- Azure SQL database, Azure SQL Managed Instance, Azure Synapse Analytics
- Dynamic data masking
  - Azure web portal through Dynamic data masking blade
  - powerdshell or REST API
- Data Masking policy
  - SQL users excluded from masking
  - masking rules
  - masking functions

#### Azure Data Encryption at Rest

- Encryption Models
  - Client-side
  - Server-side
  - Disk
  - storage service

[back to top](#top)

### Machine Learning Orchestration & Deployment

#### ML pipeline

- Pipeline steps
  - calculate requirements
  - assess dependencies between steps
  - process nodes in execution graph
- activities
- Dependency analysis
- common ML pipeline errors
  - unable to pass data to pielineData directory
  - dependency bugs
  - ambiguous erros with compute targets
  - pipeline not reusing steps
  - pipeline re-running unnecessarily
  - step slow over training epochs
  - compute target slow to start
- troubeshooting pipeline
  - debug pipeline steps
  - using logging and application insights
  - use a remote debugger
  - local script debugging
    - attach custom debug configuration
    - inspect object state
    - identify type or logical errors

#### Model Features & Differential Privacy

- explainers: to determine the importance of various features from the perspective a trained model
  - quantify influence in training data
  - label pridiction
- common explainers types
  - MimicExplainer
  - TabularExplainer
  - PFIExplainer(Permutation Feature Importance)
- Global feature importance
  - `explain_global()`, `get_feature_importance_dict()`
- Local feature importance
  - `explain_local()`, `get_ranked_local_names()`, `get_ranked_local_values()`

#### ML model Bias

- Fairlearn是一个Python软件包，可让人工智能（AI）系统开发人员评估其系统的公平性并减轻任何观察到的不公平问题
- Fairlearn 库由两个主要部分组成：
  - fairlearn.metrics：用于评估哪些群体的权益受到了侵害，并根据各种公平性规则比较模型的各个指标「例如真阳性率，选择率等等」
  - 去偏算法：有三个具体的包 fairlearn.postprocessing, fairlearn.preprocessing, fairlearn.reductions

[back to top](#top)

### Machine Learning Model Monitoring- Insights



### Azure Data Storage Monitoring

- Azure Monitor: collect, analyze, react
- Azure Monitor for storage:
  - capacity, availability, performance
  - view details, multiple storage accounts, identify issues
  - overview workbook: suscriptions, storage accounts, time range
  - capacity workbook: total used storage, used capacity by data service
  - performance metrics: health, transactions, availablity and latency


#### Azure log analytics Agent

- collects telemetry from windows and linux VMs
- Provides data to log analytics workspace
- supports insights and services
- where log analytics Agent sends data
  - to log ana lytics workspace in Azure monitor
  - windows agent can send to multiple workspace(4)
  - linux agent sends to one destination

#### Azure Monitor

- Application Insights
- Azure Monitor for Azure cache for redis, cosmos db, Key vault, Networks storage
- Container Insights
- VM insights
- Core Solutions: Agent health, Alert management, service map
- Collection from Non-Azure resources: Applications, REST API clients, VMs
- Continuous Monitoring:
  - Health, Performance, Reliability
  - Applications, Infrastructure, Azure resource groups, Continuous deployment
- Visualization Methods: Workbooks, Azure dashboards, Power BI, Grafana, Custom code

#### queries in Azure log analytics

- Azure log analytics workspace

```shell
# Kusto query language(KQL)
search in (AzureDiagnostics) "response"
| top 5 by TimeGenerated

AzureDiagnostics
| where Resource =~ "request"
| top 5 by TimeGenerated

AzureDiagnostics
| top 5 by TimeGenerated
| project TimeGenerated, Resource, status_s  # define which column should show

AzureDiagnostics
| summarize count()

AzureDiagnostics
| summarize event_count=count() by status_s
| render columnchart                        # show as chart
```

#### Montitoring Data Factory pipeline in other way

- 'Diagnostic setting' in left menu of Data factory -> 'adding Diagnostic setting' in right panel
- create 'Azure Data Factory Analytics' service
- navigate resource group -> choose 'Azure Data Factory Analytics' -> enter it
- choose 'workbooks' in review

[back to top](#top)

### Data Solution Optimization

#### Date Partitioning strategies

- implementing data partitioning
  - Horizontal(Sharding)
  - Vertical
  - Functional
- Azure data lake storage Gen2 Tuning
  - source hardware
  - network connection
- Result set caching
  - automatic caching of query results
  - results drawn from persistent cache
  - query performanced improved
  - reduction in compute resources
- Auto Optimize
  - compact small files automaticall
  - benefical for actively queried tables

#### Data Solution Optimization

- Three keys:
  - volumn:  how much data are you planning to collect and store
  - velocity:What frequencites are u collecting the data
  - variety
- strategy to develop a scalable partition
  - table entities
  - table primary key
  - table partitions
    - entity group transactions
    - range partitions
  - scalability
- example of developing a scalable partition strategy
  - analyze data
  - decide upon queries
  - partition size
  - select partitionKey

#### Azure Blob lifecycle management

- rule-based policy
- custom policy creation

#### Data Solution Optimization

- Azure Data Lake storage Gen2 Optimization
  - Improving Parallelization
    - Azure Data Factory
    - DistCp
    - Sqoop
  - Structuring data lake storage data: File size, Time series data
  - I/O intensive Jobs on Hadoop and spark or HDInsight:  CPU, memory, I/O
    - Uuing latest version of HDInsight
    - using same Region
  - Tuing HDInsight Layers
    - Physical
    - YARN container
    - workload
      - Spark: number of executors, the memory, the total number of cores
      - Hive:  container size
      - MapReduce: memory, job Maps and job Reduces
      - Storm:  total number of worker processes, spout executor instances, bolt executor instances, spout tasks and bolt tasks
- Azure stream Analytics Optimization
  - streaming units(SU)
    - reliant on partion and query
    - general, allocate more than required
    - SU percentage
      - Time-based query elements
      - Fully parallel queries
      - Event rate metrics
  - Stream analytics Jobs
    - streaming input, query, Output
  - Allocated compute resources
  - CPU and memory usages
  - Embarrassingly parallel Jobs
    - query logic and key dependence
    - partition query
    - Partion key column
    - Number of input and output partition is same
- Azure Synapse Analytics Optimization
  - Cache
  - Tuning with Ordered Clustered Columnstore Index(CCI)
  - Improving performance of complex queries
    - Standard view
    - materialized view:
      - faster execution
      - improved query execution plans
      - high availability and resilliency
      - low maintenance
      - different distribution than base tables
      - 物化视图是基于磁盘的，并根据查询定义进行定期更新
      - 视图只是虚拟的，并在每次访问时运行查询定义
- Azure Cosmos DB Optimization
  - Index Types: items to trees, trees to property paths
    - range index
    - spatial index
    - composite index
  - querying with indexes
    - extracted paths simplify index lookup
    - match WHERE with indexed paths
  - Azure Cosmos DB Partitioning(mandantary)
    - scale containers to improve performance
    - divide container items into logical partitions
    - assign items a partition key
- Azure Blob storage Optimization
  - scalability targets
  - number of storage accounts(200 accounts in one subscription, it is too much)
  - capacity and transaction targets
  - concurrent blob access
- Azure Databricks Optimization

[back to top](#top)

### High Availability & Disaster Recovery

- Concepts
  - Continuous operation
  - extended period of time
  - failure capability
  - hardware, software, connections
- metrics
  - five nines(99.9999%) uptime
- How High availability works
  - Elimate single points of failure(SPOF)
  - Ensure redundancy and failover
  - For VMs, high availability can be achieved by using clusters
  - Fault Tolerance
    - operate even during failure
    - Aims for zero downtime
    - Maintain operational capability
    - performance is not a consideration
- Disaster Recovery(DR)
  - includes policies, procedures, tools
  - Restoring a system from a secure location
  - How works
    - disaster recovery plan required
    - secondary location for backup
    - restoration should require litter intervention
- HADR(manage both High availability and disaster recovery in one seamless automated fashion)
  - make sure system distributed
  - load balancing

#### HADR and SQL Server

- Azure Geo-redundant storage(GRS)
- always on availability groups
  - primary db and up to 8 secondary db
- always on failover cluster instances(FCIs)
  - WSFC(windows Server Failover Clustering)
  - Server instance failover
  - SQL servver running on WSFC nodes
- SQL Server log shipping
  - send log backups from primary server to secondary server
  - backups applied individually
  - monitor server
  - alert when operation failed
- SQL Server backup and restore with Azure Blob storage
  - by using Azure backups
  - create file-snapshot database backup
- Database mirroring
  - deprecated after SQL Server 2016

#### Azure SQL Server backup and restore

- Automated backup
  - SQL server standard and enterprise on VMs
  - SQL Server IaaS agent Extension
  - Backups made to an Azure storage account
  - Backups stored in Recovery Services vault
- Manual Backup
  - using attached disks
  - using URL(back up to storage account)
  - Managed backup
- Azure storage for SQL Server backup
  - backup to Azure Blob storage
  - use Transact-SQL or SMO

#### Azure availability Groups

#### Azure DB for PostgreSQL availability - Hyperscale(Citus)

- Features
  - Horizontal scaling using sharding
  - query parallelization across servers
  - support for multi-tenant applications, real-time analytics
- Aspects
  - nodes
  - Coordinator and workers
- Table types with Hyperscale
  - Distributed table: partitioned across the various worker nodes
  - reference table:   all data on all of the relevant shards
  - local table:
- Application types
  - multi-tenant applications
  - real-time applications
- Table colocation
  - store related information on same node
  - ensure fast queries
  - collocate realted data on different nodes for parallel queries
- High availability
  - detection of failover
  - auto failover
  - auto recovery

[back to top](#top)

> references
- [Microsoft ML Learn - Hands On Exercises](https://microsoftlearning.github.io/mslearn-azure-ml/)
- [Azure Machine Learning examples](https://learn.microsoft.com/en-us/samples/azure/azureml-examples/azure-machine-learning-examples/)
