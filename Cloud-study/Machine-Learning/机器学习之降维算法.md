[机器学习之降维算法](#top)

- [主成分分析（Principal Component Analysis，PCA）](#主成分分析principal-component-analysispca)
- [线性判别分析（linear discriminant analysis，LDA）](#线性判别分析linear-discriminant-analysislda)
- [梯度下降算法](#梯度下降算法)


### 主成分分析（Principal Component Analysis，PCA）

- 主成分分析（Principal Component Analysis，PCA）， 是一种统计方法。通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分
- 其思想如下：
  - PCA就是将高维的数据通过线性变换投影到低维空间上去
  - 投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据
  - 去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的
  - 去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留
  - 对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵
  - 完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间
  - 之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余
​- PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将`m`维特征映射到`n维（n < m）`，这n维形成主元，是重构出来最能代表原始数据的正交特征
- PCA算法流程总结
  - 输入：n维样本集 $D=(x^{(1)},x^{(2)},...,x^{(m)})$ ,目标降维的维数 $n^l$
  - 输出：降维后的新样本集 $D^l=(z^{(1)},z^{(2)},...,z^{(m)})$


### 线性判别分析（linear discriminant analysis，LDA）

- 和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出
- LDA分类思想简单总结如下：
  - 多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理
  - 对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离
  - 对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别
- 如果用一句话概括LDA思想，即**投影后类内方差最小，类间方差最大**
- ![LDA](https://github.com/honggzb/Study-General/blob/master/Cloud-study/Machine-Learning/images/LDA.png)
  - 目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。
  - 左图思路：让不同类别的平均点距离最远的投影方式
  - ​右图思路：让同类别的数据挨得最近的投影方式
  - 右图的投影效果好于左图，左图中间直方图部分有明显交集
- LDA算法流程总结
  - ​输入：数据集 $D = (x_1,y_1),(x_2,y_2),...,(x_m,y_m)$, 其中样本$x_i$是n维向量，$y_i\in C_1,C_2,...C_k$ 降维后的目标维度d
  - 输出：降维后的数据集$\overline D$

|   |LDA|PCA|
|---|---|---|
| |1). 两者均可以对数据进行降维；||
|相同点|2). 两者在降维时均使用了矩阵特征分解的思想||
| |3). 两者都假设数据符合高斯分布||
|不同点|有监督的降维方法|无监督的降维方法|
| |降维最多降到 k-1；可以用于降维；还可以用于分类； 选择分类性能最好的投影方向；更明确，更能反映样本间差异|维降维多少没有限制；只用于降维；选择样本点投影具有最大方差的方向；目的较为模糊|
| |1). 可以使用类别的先验知识|1). 仅仅需要以方差衡量信息量，不受数据集以外的因素影响|
|优点|2). 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异|2).各主成分之间正交，可消除原始数据成分间的相互影响的因素|
| | |3). 计算方法简单，主要运算是特征值分解，易于实现|
| |1). LDA不适合对非高斯分布样本进行降维|1).主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强|
|缺点|2). LDA降维最多降到分类数k-1维|2). 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响|
| |3). LDA在样本分类信息依赖方差而不是均值时，降维效果不好||
| |4). LDA可能过度拟合数据||

[back to top](#top)

### 梯度下降算法

[梯度下降法 & 随机梯度下降](https://github.com/honggzb/Study-General/blob/master/Cloud-study/Machine-Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95--%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dstochastic-gradient-descent-sgd)

- 梯度下降法作用
  - 梯度下降是迭代法的一种，可以用于求解最小二乘问题。
  - 在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。
  - 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。
  - 如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。
  - 在机器学习中，梯度下降法主要有随机梯度(SDG)下降法和批量梯度(BDG)下降法。
- 梯度下降法缺点
  - 靠近极小值时收敛速度减慢
  - 直线搜索时可能会产生一些问题
  - 可能会“之字形”地下降
- 梯度概念也有需注意的地方：
  - 梯度是一个向量，即有方向有大小
  - 梯度的方向是最大方向导数的方向
  - 梯度的值是最大方向导数的值
- ![梯度下降算法](https://github.com/honggzb/Study-General/blob/master/Cloud-study/Machine-Learning/images/梯度下降算法.png))
  - 初始化参数，随机选取取值范围内的任意数
  - 迭代操作： a）计算当前梯度； b）修改新的变量； c）计算朝最陡的下坡方向走一步； d）判断是否需要终止，如否，返回a）
  - 得到全局最优解或者接近全局最优解
- 对梯度下降法进行调优
  1. 算法迭代步长α选择。 在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长
  2. 参数的初始值选择。 初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值
  3.标准化处理。 由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间
- 各种梯度下降法性能比较: 随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD

| 算法 |训练集|单次迭代样本数|算法复杂度|时效性|收敛性|
|---|---|---|---|---|---|
|BGD|固定|整个训练集|高|低|稳定|
|SGD|固定|单个样本|低|一般|不稳定|
|Mini-batch GD|固定|训练集的子集|一般|一般|较稳定|
|Online GD|实时更新|根据具体算法定|低|高|不稳定|

[back to top](#top)

> [随笔分类 - 机器学习算法](https://www.cnblogs.com/ai-learning-blogs/category/1352736.html)
