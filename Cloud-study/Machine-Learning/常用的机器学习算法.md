## [常用的机器学习算法](#top)

- [Types of ML](#types-of-ml)
- [The variables using a dataset](#the-variables-using-a-dataset)
- [Types of ML Models](#Types-of-ML-Models)
- [线性回归 Linear Regression](#线性回归-linear-regression)
- [逻辑回归 Logistic regression](#逻辑回归-logistic-regression)
- [线性判别分析LDA(Linear Discriminant Analysis)](#线性判别分析ldalinear-discriminant-analysis)
- [决策树(Decision Tree)](#决策树decision-tree)
- [朴素贝叶斯(Naive Bayes)](#朴素贝叶斯naive-bayes)
- [支持向量机(Support Vector Machine-SVM)](#支持向量机support-vector-machine-svm)
- [K-近邻算法(k-Nearest Neighbor)](#k-近邻算法k-nearest-neighbor)

### Types of ML

- traditional ML
- Deep Learning
- Supervised:
  - data is labelled
  - there is an input variable 'X' or set of input variables and an output variable 'Y'
  - The function is approximated to predict new values of Y given X, such as 
    - Regression: output variable is real value likes amount, height, weight etc
    - Classification: output variable is a category, such as Yes, No, Red, Blue, Yellow etc
- Unsupervised
  - only X or input variable is known
  - the goal is to modle the underlying struture or distribution in the data in order to learn more about the data
  - There is no correct answers and there is no teacher
  - Algorithms are left on their own to discover and present the interesting structure in the data
  - examples
    - Clustering: customer behaviour grouping
    - Association: Recommendation model
- Reinforcement Learning
  - rewards good behavior and penalizes bad ones
  - The idea is to maximise the gain ore reward

### The variables using a dataset

![variablesByUsingDataset](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/variablesByUsingDataset.png)

### Types of ML Models

- Classification
  - Identification of category of data
  - binary/two-class classification(either/or, yes/no type)
  - Multi-classs classification - one to many
- Regression
  - estimating the realtionships among variables
  - Predictor is a continuous variable
  - examples:  predicting the future sale of products
- Clustering or Cluster Analysis
  - Clustering is the task of grouping a set of objects in such a way that
    - objects in the same group are
    - more similar( insome sense or another
    - to each other than
    - to thos in other groups(clusters)
  - Unsupervised learning model
  - customers who make lot of long-distance calls and don't have a job. Who are they?
- Anomaly Detection
  - Identification of items, events or observations which do not conform to an expected pattern or other items in a dataset
  - Typically the anomalous items will translate to some kind of problem such as fraud, defect, problems
  - Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions

[back to top](#top)

### 线性回归 Linear Regression

- 线性回归(Linear Regression)是目前机器学习算法中最流行的一种，线性回归算法就是要找一条直线，并且让这条直线尽可能地拟合中的数据点。它试图通过将直线方程与该数据拟合来表示自变量(x)和数值结果(y)。然后就可以用这条直线来预测未来的值
- 线性回归模型被表示为一个方程式，它为输入变量找到特定的权重（即系数 B），进而描述一条最佳拟合了输入变量（x）和输出变量（y）之间关系的直线
  - `$y = B0+B1*x$`, 在给定输入值`x`的条件下预测`y`，线性回归学习算法的目的是找到系数`B0`和`B1`的值
  - ![线性回归](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)
- 可以使用不同的技术来从数据中学习线性回归模型，例如普通最小二乘法的线性代数解和梯度下降优化, 最常用的技术是最小二乘法(Least of squares)。这个方法计算出最佳拟合线，以使得与直线上每个数据点的垂直距离最小。总距离是所有数据点的垂直距离的平方和。其思想是通过最小化这个平方误差或距离来拟合模型
- 线性回归大约有 200 多年的历史，并已被广泛地研究。在使用此类技术时，有一些很好的经验规则：我们可以删除非常类似（相关）的变量，并尽可能移除数据中的噪声。线性回归是一种运算速度很快的简单技术，也是一种适合初学者尝试的经典算法

### 逻辑回归 Logistic regression

- 是二分类问题的首选方法, 应用于如广告点击预测、垃圾邮件识别、金融贷款发放等二分类问题
- 逻辑回归的目的也是找到每个输入变量的权重系数值。但不同的是，逻辑回归的输出预测结果是通过一个叫作「逻辑函数」的非线性函数变换而来的
- 逻辑函数的形状看起来像一个大的S，它会把任何值转换至 0-1 的区间内。这十分有用，因为我们可以把一个规则应用于逻辑函数的输出，从而得到 0-1 区间内的捕捉值（例如，将阈值设置为 0.5，则如果函数值小于 0.5，则输出值为 1），并预测类别的值
- 逻辑, Sigmoid函数: $g(z) = \frac{1}{1+e^{-z}}$
- 由于模型的学习方式，逻辑回归的预测结果也可以用作给定数据实例属于类 0 或类 1 的概率。
- 与线性回归类似，当删除与输出变量无关以及彼此之间非常相似（相关）的属性后，逻辑回归的效果更好。该模型学习速度快，对二分类问题十分有效
- ![逻辑回归](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/逻辑回归.png)
- ![逻辑回归案例](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/逻辑回归案例.png)

### 线性判别分析LDA(Linear Discriminant Analysis)

- 给定训练样本，设法将样本投影到一条直线上，使得同类样例的投影点尽可能的接近，异类样例的投影点尽可能的远。在对新样本进行分类时，将其投影到同样的这条直线上，再根据新样本投影点的位置确定类别
- 线性判别分析是一种很重要的分类算法，同时也是一种降维方法。LDA也是通过投影的方式达到去除数据之间冗余的一种算法。如下图所示的2类数据，为了正确的分类，希望这2类数据投影之后，同类的数据尽可能的集中（距离近，有重叠），不同类的数据尽可能的分开（距离远，无重叠）
- ![线性判别分析](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/线性判别分析.png)

[go to top](#top)

### 决策树(Decision Tree)

- 常用于研究类别归属和分类任务, 在人工智能中，决策树可用于解决分类问题。根节点包含了样本的全集，每个分枝点代表对某一特征属性的一次测试，每条边代表一个测试结果，叶子顶点代表某个类或类的分布。简单地说，决策树可以被视作是一个if-then规则的集合。
- 这个过程的关键就是建立决策树，通常的过程是：递归地选择最优特征、并用最优特征生成顶点对数据集进行分割。
- 决策树算法根据数据的属性采用树状结构建立决策模型, 根据一些feature进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上
  - 左右分支代表可能的答案。最终节点（即叶节点）对应于一个预测值。
  - 每个特征的重要性是通过自顶向下方法确定的, 节点越高，其属性就越重要
  - ![决策树](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/决策树.png)
- 常见的算法包括：分类及回归树（Classification And Regression Tree，CART），ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）

### 朴素贝叶斯(Naive Bayes)

- **贝叶斯方法**算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators，AODE），以及Bayesian Belief Network（BBN）
- 朴素贝叶斯(Naive Bayes)是基于贝叶斯定理。它测量每个类的概率，每个类的条件概率给出x的值。这个算法用于分类问题，得到一个二进制“是”/“非”的结果
  - $P(Y|X) = \frac{P(Y)P(X|Y)}{P(X)}$  - 贝叶斯定理
  - P(Y|X)代表X事件发生的条件下，Y事件发生的概率；P(X)和P(Y)分别代表了事件X和事件Y发生的概率；P(X|Y)代表了Y事件发生的条件下，X事件发生的概率
- 如用于过滤垃圾邮件

> 补充
- 联合概率: $P(A,B) = P(A)*P(B)$
- 条件概率: $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ , 贝叶斯定理(B条件下A发送的概率)
  - $P(A,B) = P(A|B)*P(B) = P(B|A)*P(A)$
- ![贝叶斯定理](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/贝叶斯定理.png)
- ![朴素贝叶斯-垃圾邮件过滤](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/朴素贝叶斯-垃圾邮件过滤.png)

[go to top](#top)

### 支持向量机(Support Vector Machine-SVM)

- 支持向量机(SVM)是一种用于分类问题的算法。
- 建立一个最优决策超平面，使得该平面两侧距平面最近的两类样 本之间的距离最大化，从而对分类问题提供良好的泛化力（推广能力） “支持向量”：则是指训练集中的某些训练点，这些点最靠近分类决策面，是最难分类的数据点
- 超平面与最近的类点之间的距离称为边距Margin。最优超平面具有最大边界，可以对点进行分类，从而使最近的数据点与这两个点间的距离最大化，使得类类间的距离最小，类间的距离最大
- ![支持向量机](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/支持向量机.png)

### K-近邻算法(k-Nearest Neighbor)

- 其原理是找出挨着自己最近的K个邻居，并且根据邻居的类别来确定自己的类别情况
- KNN 通过在整个训练集中搜索K个最相似的实例，即K个邻居，并为所有这些K个实例分配一个公共输出变量，来对对象进行分类
-  K的选择很关键：较小的值可能会得到大量的噪声和不准确的结果，而较大的值是不可行的。它最常用于分类，但也适用于回归问题
-  ![KNN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN.png)
-  用于评估实例之间相似性的距离可以是欧几里得距离（Euclidean distance）、曼哈顿距离（Manhattan distance）或明氏距离（Minkowski distance）。欧几里得距离是两点之间的普通直线距离。它实际上是点坐标之差平方和的平方根
- [相似性度量中用到的一些距离函数](https://www.cnblogs.com/belfuture/p/5871452.html)
- ![KNN参数值](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN参数值.png)

> reference
- [AI-Expert-Roadmap on github](https://github.com/AMAI-GmbH/AI-Expert-Roadmap)
- [AI-Expert-Roadmap](https://github.com/AMAI-GmbH/AI-Expert-Roadmap)
