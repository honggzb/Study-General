## [机器学习基础](#top)

- [Data of ML](#data-of-ml)
  - [Dataset Manipulation](#dataset-manipulation)
  - [Labeled and unlabeled Data](#labeled-and-unlabeled-data)
  - [Data Features](#data-features)
- [types of ML](#types-of-ml)
  - [Supervised-监督学习 ML](#supervised-监督学习-ml)
  - [Unsupervised-无监督学习 ML](#Unsupervised-无监督学习-ml)
  - [半监督学习](#半监督学习)
  - [强化学习](#强化学习)
- [ML Algorithms](#ml-algorithms)
  - [Regression Algorithms](#regression-algorithms)
  - [Classification Algorithms](#classification-algorithms)
  - [Clustering Algorthms](#clustering-algorthms)
- [常见的ML算法](#常见的ml算法)
  - [线性回归 Linear Regression](#线性回归-linear-regression)
  - [逻辑回归 Logistic regression](#逻辑回归-logistic-regression)
  - [线性判别分析LDA(Linear Discriminant Analysis)](#线性判别分析ldalinear-discriminant-analysis)
  - [决策树(Decision Tree)](#决策树decision-tree)
  - [朴素贝叶斯(Naive Bayes)](#朴素贝叶斯naive-bayes)
  - [支持向量机(Support Vector Machine-SVM)](#支持向量机support-vector-machine-svm)
  - [K-近邻算法(k-Nearest Neighbor)](#k-近邻算法k-nearest-neighbor)
  - [K-平均算法(K-Means)](#k-平均算法k-means)
  - [随机森林算法(Random Forest)](#随机森林算法random-forest)
  - [降维算法(Dimensional Reduction)](#降维算法dimensional-reduction)
  - [梯度增强算法(Gradient Boosting)](#梯度增强算法gradient-boosting)
  - [梯度下降法 \& 随机梯度下降(Stochastic Gradient Descent-SGD)](#梯度下降法--随机梯度下降stochastic-gradient-descent-sgd)
  - [神经网络-Neural Network](#神经网络-neural-network)

## Data of ML

### Dataset Manipulation

- combine datasets
- create categories for grouping
- modify columns
- fill in missing values

### Labeled and unlabeled Data

- understand Data
  - organized information
  - rows represent data points
  - columns represent features
- Data types for ML
  - unlabeled data(raw data)
    - easily obtained
    - no features
  - labeled data
    - tagged with at least one label
    - adds meaning to data
    - labeling requires human interpretation
    - more complex and expensive than row data
- ML models are applied to labeled data
- ML models can predict outcomes in unlabeled data

### Data Features

- Features in Data
  - represent meaningfull data
  - also know as attributes
  - can be analyzed
  - can be very diverse
- how features fit into ML Process
  - Data -> Features -> Model -> insights

[back to top](#top)

## types of ML

![Supervised-ML](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/ML常见算法.png)

### Supervised-监督学习 ML

- Supervised给定一组样本(通常由人工标注)，它可以学会将输入数据映射到已知目标, 也叫标注(annotation), 输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果
- 在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率
- ![Supervised-ML](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/Supervised-ML.png)
- 监督式学习的常见应用场景如分类问题和回归问题。常见算法有
  1. 序列生成(sequence generation)。给定一张图像，预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题，比如反复预测序列中的单词或标记
  2. 语法树预测(syntax tree prediction)。给定一个句子，预测其分解生成的语法树
  3. 目标检测(object detection)。给定一张图像，在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题(给定多个候选边界框，对每个框内的目标进行分类)或分类与回归联合问题(用向量回归来预测边界框的坐标)
  4. 图像分割(image segmentation)。给定一张图像，在特定物体上画一个像素级的掩模(mask)
  5. 逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）

### Unsupervised-无监督学习 ML

- 无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，在学习中数据并不被特别标识，学习模型是为了推断出数据的一些内在结构 ,其目的在于数据可视化、数据压缩、数据去噪或更好地理解数据中的相关性
  - no correct answers
  - goal is to model structure
  - algorithms have no help
- 常见的应用场景包括关联规则(association)的学习以及聚类(clustering)等:
  a. 聚类算法, k-平均算法(k-means), 分层聚类算法, 最大期望算法(EM)
  b. 可视化与降维, 主成分分析(PCA), 核主成分分析, 局部线性嵌入, t-分布随机近临嵌入
  c. 关联规则学习, Apriori, Eclat

|   | Unsupervised  |Supervised |
|---|---|---|
| Data  |raw data |labeled data|
|Complexity|computationally complex|computationally simpler|
|Accuracy|less accurate|More accurate|

### 半监督学习

- 半监督学习输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测
- 应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。

### 强化学习

- 在强化学习中，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整
- ![强化学习](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/强化学习.png)
- 常见的应用场景AlphaGo学习下棋，机器人学习行走等
- 常见算法包括Q-Learning以及时间差学习（Temporal difference learning）

[back to top](#top)

## ML Algorithms

### Regression Algorithms

- predictive modeling
- relationships between dependent and independent variables
- using in such as forecasting, identifying causal relationships, time series modeling
- benefits
  - analyze relationships
  - identify effect of independent variables
  - compare effect of scalings
- Azure regression
  - supervised ML
  - Predict label based on features
  - Train model with features and known values
  - Use trained model to predict labels for unknown items

| Algorithm  | description |
|---|---|
|   | linear realtionship between dependent and independent variables |
|Linear Regression|dependent variables is continuous, independent variables continuous or discrete|
| |determine best fit(straight line)|
| |determine probability of success or failure|
|Logistic Regression|dependent variables is binary|
| |polular for classification problems|
|Polynomial regression|regression equation|power o independent variable greater than 1|
|stepwise regression|presence of multiple independent variables|
| |selection is automated|
| |highly correlationed independent variables|
|Ridge regression|adds bias to regression estimates|
| |reduces occurrence of errors|
|  |Reduces variablity|
|Losso regression |improves accuracy of linear models|
| |Penalizes absolute size of coefficients |
|  |Hybid of Lasso and Ridge regression|
|ElasticNet regression |used for multiple correlated features|
| |No limitation on variable selections |

### Classification Algorithms

- obtain better boundary conditions
- identify target classes
- predict target class
- examples
  - classify vegetables based on shape, color, taste,
  - predict consumer purchasing decisions
  - classify cats or dogs based on shape of snout
- Classification Terms
  - classifier
  - features
  - Multi-class classification
  - classification model
  - binary classification
  - multi-label classification
- common classification Algorithms
  - linear classifiers
  - Quadratic classifiers
  - Neural networks
  - decision trees

### Clustering Algorthms

- group objects in a cluster
- some sort of similarity
- Features
  - objects in a cluster share similarities
  - objects in different clustesrs are different
  - similarity is represented by **proximity**
- Clustering methods
  - Hierarchical
  - assign points
- Types of clustering
  - centroid-based clustering
    - non-hierarchical clusters
    - very efficient
    - reliant on starting conditions
  - hierarchical clustering
    - userful for hierarchical datasets - tree of clusters
    - any number of clusters can be selected
  - density-based clustering
    - connects areas of high example density
    - arbitrary shaped distributions
    - does not assign outliers
    - problematic with varying density and high dimentsions
  - distribution-based clustering
    - data comprised of distributions
    - probability decreases as distance from center of distribution grows

[back to top](#top)

## 常见的ML算法

### 线性回归 Linear Regression

- 线性回归(Linear Regression)是目前机器学习算法中最流行的一种，线性回归算法就是要找一条直线，并且让这条直线尽可能地拟合中的数据点。它试图通过将直线方程与该数据拟合来表示自变量(x)和数值结果(y)。然后就可以用这条直线来预测未来的值
- 线性回归模型被表示为一个方程式，它为输入变量找到特定的权重（即系数 B），进而描述一条最佳拟合了输入变量（x）和输出变量（y）之间关系的直线
  - `$y = B0+B1*x$`, 在给定输入值`x`的条件下预测`y`，线性回归学习算法的目的是找到系数`B0`和`B1`的值
  - ![线性回归](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)
- 可以使用不同的技术来从数据中学习线性回归模型，例如普通最小二乘法的线性代数解和梯度下降优化, 最常用的技术是最小二乘法(Least of squares)。这个方法计算出最佳拟合线，以使得与直线上每个数据点的垂直距离最小。总距离是所有数据点的垂直距离的平方和。其思想是通过最小化这个平方误差或距离来拟合模型
- 线性回归大约有 200 多年的历史，并已被广泛地研究。在使用此类技术时，有一些很好的经验规则：我们可以删除非常类似（相关）的变量，并尽可能移除数据中的噪声。线性回归是一种运算速度很快的简单技术，也是一种适合初学者尝试的经典算法

### 逻辑回归 Logistic regression

- 是二分类问题的首选方法, 应用于如广告点击预测、垃圾邮件识别、金融贷款发放等二分类问题
- 逻辑回归的目的也是找到每个输入变量的权重系数值。但不同的是，逻辑回归的输出预测结果是通过一个叫作「逻辑函数」的非线性函数变换而来的
- 逻辑函数的形状看起来像一个大的S，它会把任何值转换至 0-1 的区间内。这十分有用，因为我们可以把一个规则应用于逻辑函数的输出，从而得到 0-1 区间内的捕捉值（例如，将阈值设置为 0.5，则如果函数值小于 0.5，则输出值为 1），并预测类别的值
- 逻辑, Sigmoid函数: $g(z) = \frac{1}{1+e^{-z}}$
- 由于模型的学习方式，逻辑回归的预测结果也可以用作给定数据实例属于类 0 或类 1 的概率。
- 与线性回归类似，当删除与输出变量无关以及彼此之间非常相似（相关）的属性后，逻辑回归的效果更好。该模型学习速度快，对二分类问题十分有效
- ![逻辑回归](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/逻辑回归.png)
- ![逻辑回归案例](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/逻辑回归案例.png)

### 线性判别分析LDA(Linear Discriminant Analysis)

- 给定训练样本，设法将样本投影到一条直线上，使得同类样例的投影点尽可能的接近，异类样例的投影点尽可能的远。在对新样本进行分类时，将其投影到同样的这条直线上，再根据新样本投影点的位置确定类别
- 线性判别分析是一种很重要的分类算法，同时也是一种降维方法。LDA也是通过投影的方式达到去除数据之间冗余的一种算法。如下图所示的2类数据，为了正确的分类，希望这2类数据投影之后，同类的数据尽可能的集中（距离近，有重叠），不同类的数据尽可能的分开（距离远，无重叠）
- ![线性判别分析](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/线性判别分析.png)

[go to top](#top)

### 决策树(Decision Tree)

- 常用于研究类别归属和分类任务, 在人工智能中，决策树可用于解决分类问题。根节点包含了样本的全集，每个分枝点代表对某一特征属性的一次测试，每条边代表一个测试结果，叶子顶点代表某个类或类的分布。简单地说，决策树可以被视作是一个if-then规则的集合。
- 这个过程的关键就是建立决策树，通常的过程是：递归地选择最优特征、并用最优特征生成顶点对数据集进行分割。
- 决策树算法根据数据的属性采用树状结构建立决策模型, 根据一些feature进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上
  - 左右分支代表可能的答案。最终节点（即叶节点）对应于一个预测值。
  - 每个特征的重要性是通过自顶向下方法确定的, 节点越高，其属性就越重要
  - ![决策树](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/决策树.png)
- 常见的算法包括：分类及回归树（Classification And Regression Tree，CART），ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）

### 朴素贝叶斯(Naive Bayes)

- **贝叶斯方法**算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators，AODE），以及Bayesian Belief Network（BBN）
- 朴素贝叶斯(Naive Bayes)是基于贝叶斯定理。它测量每个类的概率，每个类的条件概率给出x的值。这个算法用于分类问题，得到一个二进制“是”/“非”的结果
  - $P(Y|X) = \frac{P(Y)P(X|Y)}{P(X)}$  - 贝叶斯定理
  - P(Y|X)代表X事件发生的条件下，Y事件发生的概率；P(X)和P(Y)分别代表了事件X和事件Y发生的概率；P(X|Y)代表了Y事件发生的条件下，X事件发生的概率
- 如用于过滤垃圾邮件

> 补充
- 联合概率: $P(A,B) = P(A)*P(B)$
- 条件概率: $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ , 贝叶斯定理(B条件下A发送的概率)
  - $P(A,B) = P(A|B)*P(B) = P(B|A)*P(A)$
- ![贝叶斯定理](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/贝叶斯定理.png)
- ![朴素贝叶斯-垃圾邮件过滤](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/朴素贝叶斯-垃圾邮件过滤.png)

[go to top](#top)

### 支持向量机(Support Vector Machine-SVM)

- 支持向量机(SVM)是一种用于分类问题的算法。
- 建立一个最优决策超平面，使得该平面两侧距平面最近的两类样 本之间的距离最大化，从而对分类问题提供良好的泛化力（推广能力） “支持向量”：则是指训练集中的某些训练点，这些点最靠近分类决策面，是最难分类的数据点
- 超平面与最近的类点之间的距离称为边距Margin。最优超平面具有最大边界，可以对点进行分类，从而使最近的数据点与这两个点间的距离最大化，使得类类间的距离最小，类间的距离最大
- ![支持向量机](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/支持向量机.png)

### K-近邻算法(k-Nearest Neighbor)

- 其原理是找出挨着自己最近的K个邻居，并且根据邻居的类别来确定自己的类别情况
- KNN 通过在整个训练集中搜索K个最相似的实例，即K个邻居，并为所有这些K个实例分配一个公共输出变量，来对对象进行分类
-  K的选择很关键：较小的值可能会得到大量的噪声和不准确的结果，而较大的值是不可行的。它最常用于分类，但也适用于回归问题
-  ![KNN](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN.png)
-  用于评估实例之间相似性的距离可以是欧几里得距离（Euclidean distance）、曼哈顿距离（Manhattan distance）或明氏距离（Minkowski distance）。欧几里得距离是两点之间的普通直线距离。它实际上是点坐标之差平方和的平方根
- [相似性度量中用到的一些距离函数](https://www.cnblogs.com/belfuture/p/5871452.html)
- ![KNN参数值](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/KNN参数值.png)

[go to top](#top)

### K-平均算法(K-Means)

-  K-Means算法把 n 个点（可以是样本的一次观察或一个实例）划分到 k 个集群（cluster），使得每个点都属于离他最近的均值（即聚类中心，centroid）对应的集群。重复上述过程一直持续到重心不改变
-  ![K-Means](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/K-Means.png)

### 随机森林算法(Random Forest)

- 随机森林可以看作一个决策树的集合。
- 随机森林中每棵决策树估计一个分类，这个过程称为“投票（vote）”。理想情况下，根据每棵决策树的每个投票，选择最多投票的分类
- ![随机森林](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/随机森林.png)

### 降维算法(Dimensional Reduction)

- 降维是指在限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程，并可进一步细分为特征选择和特征提取两大方法
- 一些数据集可能包含许多难以处理的变量。特别是资源丰富的情况下，系统中的数据将非常详细。在这种情况下，数据集可能包含数千个变量，其中大多数变量也可能是不必要的。在这种情况下，几乎不可能确定对我们的预测影响最大的变量。此时需要使用降维算法，降维的过程中也可能需要用到其他算法，例如借用随机森林，决策树来识别最重要的变量

### 梯度增强算法(Gradient Boosting)

- 使用多个弱算法来创建更强大的精确算法。它与使用单个估计量不同，而是使用多个估计量创建一个更稳定和更健壮的算法。梯度增强算法有几种：
  - XGBoost  — 使用线性和树算法
  - 梯度增强算法的特点是精度较高。此外，LightGBM 算法具有令人难以置信的高性能

#### 梯度下降法 & 随机梯度下降(Stochastic Gradient Descent-SGD)

- 通过迭代找到目标函数的最小值
- 梯度是一个矢量，告诉权重的方向。更准确地说，如何改变权重，使损失变化最快。这个过程为梯度下降，因为它使用梯度使损失曲线下降到最小值。
- 随机的意思是由偶然决定的。训练是随机的，因为小批量是数据集中的随机样本。这就是为什么它被称为SGD
- 如果能成功地训练一个网络做到这一点，它的权重必须以某种方式表示这些特征和在训练数据中表示目标之间的关系。
- 除了训
- 练数据, 还需要两件事:
  - 一个损失函数(loss function)，用来衡量网络的预测有多好, 用来测量目标的真实值和模型预测值之间的差距。不同的问题需要不同的损失函数
    - 一个常见的损失函数回归问题是平均绝对误差(MAE)。对于每个预测y_pred, MAE通过绝对差异abs(y_true - y_pred)衡量与真实目标y_true的差异, 数据集上的MAE总损失是所有这些绝对差异的平均值
    - 均方误差(Mean Square Error-MSE)
    - Huber损失
  - 一个优化器（optimizer），是一种调整权重以使损失最小化的算法, 告诉网络如何改变其权重
- [梯度下降法 & 随机梯度下降（SGD）](https://zhuanlan.zhihu.com/p/371320355)
- [机器学习，梯度下降算法，问题引入-youtube](https://www.youtube.com/watch?v=M3U7-uCIPjI)
- [机器学习，梯度下降算法，数学原理](https://www.youtube.com/watch?v=UkcUZTe49Pg)
- [梯度下降算法的设计与实现](https://www.bilibili.com/video/BV1Df4y1u7K4/?spm_id_from=333.999.0.0&vd_source=0db23696e276956f73c1f7d48e7825eb)

[go to top](#top)

#### 神经网络-Neural Network

- 深度学习可以理解为神经网络。刚开始只有神经网络的概念，随着神经网络的层数增加，就逐渐将神经网络叫做深度学习
- 深度学习特点：与传统的机器学习算法、浅层神经网络相比，现代的深度学习算法通常数据量大、计算力强、网络规模大、通用智能
- 深度学习应用：计算机视觉、自然语言处理、强化学习等等。
- 深度学习框架：
  - PyTorch是Facebook 基于原 Torch 框架推出的采用 Python 作为主要开发语言的深度学习框架, 具有精简灵活的接口设计，可以快速搭建和调试网络模型
  - TensorFlow是Google于 2015 年发布的深度学习框架，2019年Google推出TensorFlow 2正式版本，以动态图优先模式运行，在工业界拥有完备的解决方案和用户基础
- 神经网络属于机器学习的一个研究分支，它特指利用多个神经元去参数化映射函数的模型
- 一个神经网络的搭建，需要满足三个条件。
  - ![神经网络的运作过程](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/神经网络的运作过程.png)
  - 输入和输出
  - 权重（w）和阈值（b）
  - 多层感知器的结构
- 神经网络模型的训练
  - ![神经网络的运作过程](https://github.com/honggzb/Study-General/blob/master/Cloud-study/images/神经网络模型的训练.png)
  - 就是试错法。其他参数都不变，w（或b）的微小变动，记作Δw（或Δb），然后观察输出有什么变化。不断重复这个过程，直至得到对应最精确输出的那组w和b，就是我们要的值
- 神经网络的运作过程
  1. 确定输入和输出
  2. 找到一种或多种算法，可以从输入得到输出
  3 找到一组已知答案的数据集，用来训练模型，估算w和b
  4. 一旦新的数据产生，输入模型，就可以得到结果，同时对w和b进行校正
- 神经网络模型中激活函数(Activation Function)
  - Sigmoid函数: $\frac{1}{1+e^{-x}}$
  - Tanh函数:   $\frac{e^x-e{-x}}{e^x+e^{-x}}$
  - ReLU函数:   非线性函数
  - Leaky ReLU函数

> [神经网络入门](http://www.ruanyifeng.com/blog/2017/07/neural-network.html)
> [【数之道 06】神经网络模型中激活函数的选择](https://www.youtube.com/watch?v=pyB65Cwx9eE)

[go to top](#top)

> references
- [AI-Expert-Roadmap on github](https://github.com/AMAI-GmbH/AI-Expert-Roadmap)
- [AI-Expert-Roadmap](https://github.com/AMAI-GmbH/AI-Expert-Roadmap)
- [机器学习常见算法分类汇总](https://www.cnblogs.com/wangsongbai/p/11088724.html)
- [机器学习的分类](https://zhuanlan.zhihu.com/p/159323858?ivk_sa=1024320u)


